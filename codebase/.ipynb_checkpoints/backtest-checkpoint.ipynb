{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118700a1-3e32-47d2-b97a-8dc733840b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from datetime import datetime as dt\n",
    "import datetime as datetime\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import time\n",
    "import traceback\n",
    "import gc\n",
    "from itertools import product\n",
    "import sys\n",
    "from scipy import sparse\n",
    "import pickle\n",
    "import uuid\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29886cd7-fe60-4d9b-930e-2c4b574a3518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import numpy as np\n",
    "\n",
    "class Trade:\n",
    "    \"\"\"\n",
    "    Class to represent a pair trade with all relevant information.\n",
    "    \n",
    "    Tracks entry and exit information, calculates PnL, and handles financing costs.\n",
    "    \"\"\"\n",
    "    # Valid trading sides\n",
    "    VALID_SIDES = ['short_black_long_white', 'long_black_short_white']\n",
    "    \n",
    "    # Financing cost parameters\n",
    "    DEFAULT_SHORT_SPREAD = 0.01  # 100 bps over Fed Funds\n",
    "    DEFAULT_LONG_SPREAD = 0.015  # 150 bps over Fed Funds\n",
    "    DAYS_PER_YEAR = 365\n",
    "    \n",
    "    def __init__(self, entry_date, permno_black, permno_white, side, z_diff_entry,\n",
    "                 investment_black, investment_white, shares_black, shares_white,\n",
    "                 entry_price_black, entry_price_white, entry_transaction_cost,\n",
    "                 zscore_method, horizon, lookback, \n",
    "                 short_spread=DEFAULT_SHORT_SPREAD, \n",
    "                 long_spread=DEFAULT_LONG_SPREAD):\n",
    "        \"\"\"Initialize a new trade.\"\"\"\n",
    "        # Validate inputs\n",
    "        self._validate_inputs(entry_date, side, investment_black, investment_white, \n",
    "                              shares_black, shares_white, entry_price_black, \n",
    "                              entry_price_white, entry_transaction_cost)\n",
    "        \n",
    "        # Generate unique trade ID\n",
    "        self.trade_id = str(uuid.uuid4())\n",
    "        \n",
    "        # Entry information\n",
    "        self.entry_date = entry_date\n",
    "        self.permno_black = permno_black\n",
    "        self.permno_white = permno_white\n",
    "        self.side = side\n",
    "        self.z_diff_entry = z_diff_entry\n",
    "        self.investment_black = investment_black\n",
    "        self.investment_white = investment_white\n",
    "        self.shares_black = shares_black\n",
    "        self.shares_white = shares_white\n",
    "        self.entry_price_black = entry_price_black\n",
    "        self.entry_price_white = entry_price_white\n",
    "        self.entry_transaction_cost = entry_transaction_cost\n",
    "        \n",
    "        # Parameters used for this trade\n",
    "        self.zscore_method = zscore_method\n",
    "        self.horizon = horizon\n",
    "        self.lookback = lookback\n",
    "        \n",
    "        # Financing parameters\n",
    "        self.short_spread = short_spread\n",
    "        self.long_spread = long_spread\n",
    "        \n",
    "        # Status tracking\n",
    "        self.status = 'open'\n",
    "        self.exit_date = None\n",
    "        self.days_held = 0\n",
    "        \n",
    "        # Exit information\n",
    "        self.exit_price_black = None\n",
    "        self.exit_price_white = None\n",
    "        self.exit_transaction_cost = None\n",
    "        self.financing_cost = 0\n",
    "        self.z_diff_exit = None\n",
    "        self.exit_reason = None\n",
    "        self.gross_pnl = None\n",
    "        self.net_pnl = None\n",
    "        \n",
    "        # Risk metrics\n",
    "        self.max_drawdown = 0.0\n",
    "        self.peak_value = self.investment_black + self.investment_white\n",
    "        self.current_value = self.peak_value\n",
    "        \n",
    "        # Track daily values for analysis\n",
    "        self.daily_values = {entry_date: self.peak_value}\n",
    "        self.daily_pnl = {}\n",
    "    \n",
    "    def _validate_inputs(self, entry_date, side, investment_black, investment_white, \n",
    "                     shares_black, shares_white, price_black, price_white, \n",
    "                     transaction_cost):\n",
    "        \"\"\"Validate inputs to ensure they make sense.\"\"\"\n",
    "        # Check side is valid\n",
    "        if side not in self.VALID_SIDES:\n",
    "            raise ValueError(f\"side must be one of {self.VALID_SIDES}, got {side}\")\n",
    "        \n",
    "        # Check investments are positive\n",
    "        if investment_black <= 0 or investment_white <= 0:\n",
    "            raise ValueError(\"Investment amounts must be positive\")\n",
    "        \n",
    "        # Check shares are positive integers\n",
    "        if not isinstance(shares_black, int) or shares_black <= 0:\n",
    "            raise ValueError(f\"shares_black must be a positive integer, got {shares_black}\")\n",
    "        if not isinstance(shares_white, int) or shares_white <= 0:\n",
    "            raise ValueError(f\"shares_white must be a positive integer, got {shares_white}\")\n",
    "        \n",
    "        # Check prices are positive\n",
    "        if price_black <= 0 or price_white <= 0:\n",
    "            raise ValueError(\"Prices must be positive\")\n",
    "        \n",
    "        # Check transaction cost is non-negative\n",
    "        if transaction_cost < 0:\n",
    "            raise ValueError(\"Transaction cost cannot be negative\")\n",
    "    \n",
    "    def update_daily_financing(self, current_date, fed_funds_rate):\n",
    "        \"\"\"Update daily financing costs for open positions.\"\"\"\n",
    "        # Calculate daily financing cost based on position side\n",
    "        if self.side == 'short_black_long_white':\n",
    "            # Short black (credit at short rate), Long white (debit at long rate)\n",
    "            black_daily_cost = -self.investment_black * (fed_funds_rate + self.short_spread) / self.DAYS_PER_YEAR\n",
    "            white_daily_cost = self.investment_white * (fed_funds_rate + self.long_spread) / self.DAYS_PER_YEAR\n",
    "        else:\n",
    "            # Long black (debit at long rate), Short white (credit at short rate)\n",
    "            black_daily_cost = self.investment_black * (fed_funds_rate + self.long_spread) / self.DAYS_PER_YEAR\n",
    "            white_daily_cost = -self.investment_white * (fed_funds_rate + self.short_spread) / self.DAYS_PER_YEAR\n",
    "        \n",
    "        daily_financing = black_daily_cost + white_daily_cost\n",
    "        self.financing_cost += daily_financing\n",
    "        \n",
    "        # Increment days held\n",
    "        self.days_held += 1\n",
    "        \n",
    "        return daily_financing\n",
    "    \n",
    "    def update_market_value(self, current_date, current_price_black, current_price_white):\n",
    "        \"\"\"Update the market value of the position and track drawdowns.\"\"\"\n",
    "        # Calculate current value of both positions\n",
    "        if self.side == 'short_black_long_white':\n",
    "            # Short black, long white\n",
    "            black_value = self.investment_black - (self.shares_black * (current_price_black - self.entry_price_black))\n",
    "            white_value = self.investment_white + (self.shares_white * (current_price_white - self.entry_price_white))\n",
    "        else:\n",
    "            # Long black, short white\n",
    "            black_value = self.investment_black + (self.shares_black * (current_price_black - self.entry_price_black))\n",
    "            white_value = self.investment_white - (self.shares_white * (current_price_white - self.entry_price_white))\n",
    "        \n",
    "        # Calculate current total value\n",
    "        current_value = black_value + white_value\n",
    "        \n",
    "        # Calculate unrealized PnL\n",
    "        unrealized_pnl = current_value - (self.investment_black + self.investment_white)\n",
    "        \n",
    "        # Update peak value if current value is higher\n",
    "        if current_value > self.peak_value:\n",
    "            self.peak_value = current_value\n",
    "        \n",
    "        # Update max drawdown if current drawdown is larger\n",
    "        current_drawdown = (self.peak_value - current_value) / self.peak_value\n",
    "        if current_drawdown > self.max_drawdown:\n",
    "            self.max_drawdown = current_drawdown\n",
    "        \n",
    "        # Store daily values\n",
    "        self.daily_values[current_date] = current_value\n",
    "        self.daily_pnl[current_date] = unrealized_pnl\n",
    "        \n",
    "        # Update current value\n",
    "        self.current_value = current_value\n",
    "        \n",
    "        return current_value\n",
    "    \n",
    "    def close_trade(self, exit_date, exit_price_black, exit_price_white, exit_reason, z_diff_exit):\n",
    "        \"\"\"Close the trade and calculate PnL.\"\"\"\n",
    "        self.exit_date = exit_date\n",
    "        self.exit_price_black = exit_price_black\n",
    "        self.exit_price_white = exit_price_white\n",
    "        self.exit_reason = exit_reason\n",
    "        self.z_diff_exit = z_diff_exit\n",
    "        \n",
    "        # Calculate transaction costs for exit (0.01 per share)\n",
    "        self.exit_transaction_cost = 0.01 * (self.shares_black + self.shares_white)\n",
    "        \n",
    "        # Calculate PnL for each leg\n",
    "        if self.side == 'short_black_long_white':\n",
    "            # Short black: profit when price falls\n",
    "            black_pnl = -self.shares_black * (exit_price_black - self.entry_price_black)\n",
    "            # Long white: profit when price rises\n",
    "            white_pnl = self.shares_white * (exit_price_white - self.entry_price_white)\n",
    "        else:  # long_black_short_white\n",
    "            # Long black: profit when price rises\n",
    "            black_pnl = self.shares_black * (exit_price_black - self.entry_price_black)\n",
    "            # Short white: profit when price falls\n",
    "            white_pnl = -self.shares_white * (exit_price_white - self.entry_price_white)\n",
    "        \n",
    "        # Calculate gross and net PnL\n",
    "        self.gross_pnl = black_pnl + white_pnl\n",
    "        \n",
    "        # Total costs include entry and exit transaction costs plus financing\n",
    "        total_costs = self.entry_transaction_cost + self.exit_transaction_cost + self.financing_cost\n",
    "        \n",
    "        # Calculate net PnL\n",
    "        self.net_pnl = self.gross_pnl - total_costs\n",
    "        \n",
    "        # Update status\n",
    "        self.status = 'closed'\n",
    "        \n",
    "        # Final update to daily values\n",
    "        self.daily_values[exit_date] = self.investment_black + self.investment_white + self.net_pnl\n",
    "        \n",
    "        # Calculate ROI\n",
    "        self.roi = self.net_pnl / (self.investment_black + self.investment_white)\n",
    "        \n",
    "        return self.net_pnl\n",
    "    \n",
    "    def to_dict(self):\n",
    "        \"\"\"Convert trade object to dictionary for logging and analysis.\"\"\"\n",
    "        return {\n",
    "            'trade_id': self.trade_id,\n",
    "            'entry_date': self.entry_date,\n",
    "            'exit_date': self.exit_date,\n",
    "            'permno_black': self.permno_black,\n",
    "            'permno_white': self.permno_white,\n",
    "            'side': self.side,\n",
    "            'z_diff_entry': self.z_diff_entry,\n",
    "            'z_diff_exit': self.z_diff_exit,\n",
    "            'investment_black': self.investment_black,\n",
    "            'investment_white': self.investment_white,\n",
    "            'shares_black': self.shares_black,\n",
    "            'shares_white': self.shares_white,\n",
    "            'entry_price_black': self.entry_price_black,\n",
    "            'entry_price_white': self.entry_price_white,\n",
    "            'exit_price_black': self.exit_price_black,\n",
    "            'exit_price_white': self.exit_price_white,\n",
    "            'entry_transaction_cost': self.entry_transaction_cost,\n",
    "            'exit_transaction_cost': self.exit_transaction_cost,\n",
    "            'financing_cost': self.financing_cost,\n",
    "            'gross_pnl': self.gross_pnl,\n",
    "            'net_pnl': self.net_pnl,\n",
    "            'roi': getattr(self, 'roi', None),\n",
    "            'exit_reason': self.exit_reason,\n",
    "            'status': self.status,\n",
    "            'days_held': self.days_held,\n",
    "            'max_drawdown': self.max_drawdown,\n",
    "            'zscore_method': self.zscore_method,\n",
    "            'horizon': self.horizon,\n",
    "            'lookback': self.lookback\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9be464c-7ac6-4813-a6d4-fa420f70abef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "class SignalGenerator:\n",
    "    def __init__(self, df_main, df_pairs, zscore_method='ou', zscore_threshold=1.5, horizon=5, lookback_period=20):\n",
    "        self.df_main = df_main\n",
    "        self.df_pairs = df_pairs\n",
    "        self.zscore_method = zscore_method\n",
    "        self.zscore_threshold = zscore_threshold\n",
    "        self.lookback_period = lookback_period\n",
    "        self.precomputed_signals = None\n",
    "        self.horizon = horizon\n",
    "        \n",
    "        # Add diagnostic print\n",
    "        print(f\"SignalGenerator initialized with {len(df_pairs)} pairs\")\n",
    "        print(f\"Z-score method: {zscore_method}, threshold: {zscore_threshold}, lookback: {lookback_period}\")\n",
    "        \n",
    "        # Check sample z-score values\n",
    "        z_col = f'z_{zscore_method}_{self.horizon}d_lb{lookback_period}'\n",
    "        if z_col in df_main.columns:\n",
    "            z_values = df_main[z_col].dropna()\n",
    "            print(f\"Z-score column '{z_col}' stats:\")\n",
    "            print(f\"  - Non-null values: {len(z_values)} out of {len(df_main)} ({len(z_values)/len(df_main)*100:.2f}%)\")\n",
    "            print(f\"  - Range: {z_values.min():.4f} to {z_values.max():.4f}\")\n",
    "            print(f\"  - Values exceeding threshold {zscore_threshold}: {(abs(z_values) >= zscore_threshold).sum()} ({(abs(z_values) >= zscore_threshold).sum()/len(z_values)*100:.2f}%)\")\n",
    "        else:\n",
    "            print(f\"WARNING: Z-score column '{z_col}' not found in data!\")\n",
    "\n",
    "    def precompute_signals_parallel(self, horizon=5, n_jobs=4):\n",
    "        \"\"\"Precompute signals for all dates and pairs in parallel\"\"\"\n",
    "        z_col = f'z_{self.zscore_method}_{horizon}d_lb{self.lookback_period}'\n",
    "        print(f\"Precomputing signals for z-score column: {z_col}\")\n",
    "        \n",
    "        # Verify z-score column exists\n",
    "        if z_col not in self.df_main.columns:\n",
    "            print(f\"ERROR: Z-score column '{z_col}' not found in data columns!\")\n",
    "            print(f\"Available columns: {self.df_main.columns}\")\n",
    "            return\n",
    "        \n",
    "        # Group by group_id for efficient processing\n",
    "        group_ids = self.df_pairs['group_id'].unique()\n",
    "        print(f\"Processing {len(group_ids)} unique group_ids\")\n",
    "        \n",
    "        chunk_size = max(1, len(group_ids) // n_jobs)\n",
    "        chunked_groups = [group_ids[i:i + chunk_size] for i in range(0, len(group_ids), chunk_size)]\n",
    "        \n",
    "        # Precompute group dictionaries\n",
    "        print(\"Building group dictionaries...\")\n",
    "        group_df_main_dict = {}\n",
    "        for group_id in group_ids:\n",
    "            group_data = self.df_main[self.df_main['group_id'] == group_id]\n",
    "            if 'permno' in group_data.columns and z_col in group_data.columns and 'date' in group_data.columns:\n",
    "                filtered_data = group_data[['permno', z_col, 'date']].dropna()\n",
    "                group_df_main_dict[group_id] = filtered_data\n",
    "                if len(filtered_data) < 10 and len(filtered_data) > 0:\n",
    "                    print(f\"  Group {group_id}: Only {len(filtered_data)} records with valid z-scores\")\n",
    "            else:\n",
    "                print(f\"  WARNING: Missing required columns for group {group_id}\")\n",
    "        \n",
    "        print(f\"Created dictionaries for {len(group_df_main_dict)} groups\")\n",
    "        \n",
    "        # Process chunks in parallel\n",
    "        all_results = []\n",
    "        signal_counts = []\n",
    "        \n",
    "        for chunk_idx, chunk in enumerate(chunked_groups):\n",
    "            # Process each group in parallel within the chunk\n",
    "            parallel_results = Parallel(n_jobs=n_jobs, prefer=\"threads\")(\n",
    "                delayed(self._process_group_signal)(\n",
    "                    group_id,\n",
    "                    group_df_main_dict,\n",
    "                    self.df_pairs[self.df_pairs['group_id'] == group_id],\n",
    "                    z_col,\n",
    "                    self.zscore_threshold,\n",
    "                    horizon\n",
    "                )\n",
    "                for group_id in chunk\n",
    "            )\n",
    "            \n",
    "            # Count signals in this chunk\n",
    "            chunk_signals = 0\n",
    "            # Append non-empty results\n",
    "            for df in parallel_results:\n",
    "                if not df.empty:\n",
    "                    all_results.append(df)\n",
    "                    chunk_signals += len(df)\n",
    "            \n",
    "            signal_counts.append(chunk_signals)\n",
    "            print(f\"Chunk {chunk_idx+1}: Generated {chunk_signals} signals\")\n",
    "        \n",
    "        # Concatenate results\n",
    "        if all_results:\n",
    "            self.precomputed_signals = pd.concat(all_results).reset_index(drop=True)\n",
    "            print(f\"Total signals generated: {len(self.precomputed_signals)}\")\n",
    "            print(f\"Signals span {self.precomputed_signals['date'].nunique()} unique trading days\")\n",
    "            \n",
    "            # Distribution of signals\n",
    "            if len(self.precomputed_signals) > 0:\n",
    "                signal_counts = self.precomputed_signals['signal'].value_counts()\n",
    "                print(f\"Signal distribution: {dict(signal_counts)}\")\n",
    "        else:\n",
    "            print(\"WARNING: No signals were generated!\")\n",
    "            self.precomputed_signals = pd.DataFrame()\n",
    "\n",
    "    def _process_group_signal(self, group_id, group_df_main_dict, df_pairs_group, z_col, zscore_threshold, horizon):\n",
    "        \"\"\"Process signals for a specific group (used for parallel processing)\"\"\"\n",
    "        if group_id not in group_df_main_dict:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        group_df_main = group_df_main_dict[group_id]\n",
    "        \n",
    "        if group_df_main.empty or df_pairs_group.empty:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Create efficient lookups\n",
    "        permnos = group_df_main['permno'].values\n",
    "        z_scores = group_df_main[z_col].values\n",
    "        dates = group_df_main['date'].unique()\n",
    "        \n",
    "        # Create lookup dictionaries\n",
    "        z_map = dict(zip(permnos, z_scores))\n",
    "        \n",
    "        # Process in chunks for memory efficiency\n",
    "        chunk_size = 1000\n",
    "        results = []\n",
    "        \n",
    "        for i in range(0, len(df_pairs_group), chunk_size):\n",
    "            df_chunk = df_pairs_group.iloc[i:i+chunk_size].copy()\n",
    "            \n",
    "            # Map z-scores efficiently\n",
    "            df_chunk['z_black'] = df_chunk['permno_black'].map(z_map)\n",
    "            df_chunk['z_white'] = df_chunk['permno_white'].map(z_map)\n",
    "            \n",
    "            # Drop rows with NaN z-scores\n",
    "            df_chunk = df_chunk.dropna(subset=['z_black', 'z_white'])\n",
    "            \n",
    "            if df_chunk.empty:\n",
    "                continue\n",
    "                \n",
    "            # Calculate z_diff\n",
    "            df_chunk['z_diff'] = df_chunk['z_black'] - df_chunk['z_white']\n",
    "            \n",
    "            # Process each date\n",
    "            for date in dates:\n",
    "                df_date = df_chunk.copy()\n",
    "                df_date['date'] = date\n",
    "                \n",
    "                # Generate signals using vectorized operations\n",
    "                z_diff_values = df_date['z_diff'].values\n",
    "                mask_short = z_diff_values >= zscore_threshold\n",
    "                mask_long = z_diff_values <= -zscore_threshold\n",
    "                \n",
    "                if not (np.any(mask_short) or np.any(mask_long)):\n",
    "                    continue\n",
    "                    \n",
    "                signals = np.full(len(z_diff_values), '', dtype=object)\n",
    "                signals[mask_short] = 'short_black_long_white'\n",
    "                signals[mask_long] = 'long_black_short_white'\n",
    "                \n",
    "                df_date['signal'] = signals\n",
    "                \n",
    "                # Filter valid signals only\n",
    "                df_date = df_date[signals != '']\n",
    "                \n",
    "                if len(df_date) > 0:\n",
    "                    # Add method info for later reference\n",
    "                    df_date['zscore_method'] = self.zscore_method\n",
    "                    df_date['horizon'] = horizon\n",
    "                    df_date['lookback'] = self.lookback_period\n",
    "                    results.append(df_date)\n",
    "            \n",
    "            # Clear memory\n",
    "            del df_chunk\n",
    "        \n",
    "        if not results:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        result_df = pd.concat(results)\n",
    "        return result_df\n",
    "\n",
    "    def generate_signals(self, date):\n",
    "        \"\"\"Get signals for a specific date\"\"\"\n",
    "        if self.precomputed_signals is None or self.precomputed_signals.empty:\n",
    "            return []\n",
    "\n",
    "        signals_today = self.precomputed_signals[self.precomputed_signals['date'] == date]\n",
    "        \n",
    "        if signals_today.empty:\n",
    "            return []\n",
    "        \n",
    "        result = signals_today[['date', 'permno_black', 'permno_white', 'signal', 'z_diff', \n",
    "                               'zscore_method', 'horizon', 'lookback']].to_dict('records')\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19008aab-69aa-4dc2-a930-c4291f679b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "#from .trade import Trade\n",
    "\n",
    "class PortfolioManager:\n",
    "    def __init__(self, df_main, initial_capital, max_holding_days=5):\n",
    "        self.df_main = df_main\n",
    "        self.initial_capital = initial_capital\n",
    "        self.available_capital = initial_capital\n",
    "        self.max_holding_days = max_holding_days\n",
    "        self.active_trades = []\n",
    "        self.trade_history = []\n",
    "        self.daily_pnl = {}\n",
    "        self.equity_curve = {pd.Timestamp.min: initial_capital}  # Initialize with starting capital\n",
    "        \n",
    "        # Create lookups for efficient access\n",
    "        self._create_lookups()\n",
    "        \n",
    "    def _create_lookups(self):\n",
    "        \"\"\"Create efficient lookups for prices and volumes\"\"\"\n",
    "        # Create lookups directly from df_main\n",
    "        lookup_data = self.df_main.set_index(['date', 'permno'])\n",
    "        self.price_lookup = lookup_data['adj_prc'].to_dict()\n",
    "        self.vol_lookup = lookup_data['adv20'].to_dict()\n",
    "        self.volatility_lookup = lookup_data['garch_vol'].to_dict()\n",
    "        \n",
    "        # Single date-indexed dataframe for other lookups\n",
    "        date_indexed = self.df_main.drop_duplicates('date').set_index('date')\n",
    "        self.ffr_lookup = date_indexed['fed_funds_rate'].to_dict()\n",
    "        self.market_return_lookup = date_indexed['vwretd'].to_dict()\n",
    "    \n",
    "    def _calculate_max_shares(self, permno, current_date, price, allocated_money=None):      \n",
    "        \"\"\"Calculate maximum number of shares based on liquidity and capital\"\"\"\n",
    "        # Default allocated money if not provided\n",
    "        if allocated_money is None:\n",
    "            allocated_money = self.available_capital / 10  # Default to 10% of available capital\n",
    "        \n",
    "        # Calculate shares based on allocated money\n",
    "        capital_shares = 0\n",
    "        if price > 0:\n",
    "            capital_shares = int(allocated_money / price)\n",
    "        \n",
    "        # Get 20-day average volume with proper error handling\n",
    "        try:\n",
    "            adv20 = self.vol_lookup.get((current_date, permno), 0)\n",
    "            \n",
    "            # Handle NaN, None, or zero values\n",
    "            if adv20 is None or np.isnan(adv20) or adv20 <= 0:\n",
    "                # If ADV20 is invalid, return capital-based shares\n",
    "                return max(1, capital_shares)\n",
    "            \n",
    "            # Limit to 10% of average volume\n",
    "            liquidity_shares = int(adv20 * 0.1)\n",
    "            \n",
    "            # Take minimum of capital-based shares and liquidity-based shares\n",
    "            max_shares = min(capital_shares, liquidity_shares)\n",
    "            \n",
    "            # Ensure at least 1 share if we have capital\n",
    "            return max(1, max_shares) if capital_shares > 0 else 0\n",
    "            \n",
    "        except (TypeError, ValueError) as e:\n",
    "            # Fall back to capital-based shares\n",
    "            return max(1, capital_shares)\n",
    "            \n",
    "    def reset_capital(self, amount):\n",
    "        \"\"\"Reset available capital (called at the start of each quarter)\"\"\"\n",
    "        self.available_capital = amount\n",
    "        \n",
    "    def process_trading_day(self, current_date, signals, current_data):\n",
    "        \"\"\"Process a single trading day\"\"\"\n",
    "        trade_updates = []\n",
    "        \n",
    "        # First update financing costs for all active trades\n",
    "        fed_funds_rate = self.ffr_lookup.get(current_date, 0.02)  # Default to 2% if missing\n",
    "        for trade in self.active_trades:\n",
    "            trade.update_daily_financing(current_date, fed_funds_rate)\n",
    "            \n",
    "            # Optional: Update market value for active trades (for internal tracking only)\n",
    "            price_black = self.price_lookup.get((current_date, trade.permno_black))\n",
    "            price_white = self.price_lookup.get((current_date, trade.permno_white))\n",
    "            \n",
    "            if price_black is not None and price_white is not None:\n",
    "                trade.update_market_value(current_date, price_black, price_white)\n",
    "        \n",
    "        # Then check for exits (z-score reversal or max holding period)\n",
    "        closed_trades = self._process_exits(current_date, current_data)\n",
    "        \n",
    "        # Update available capital from closed trades\n",
    "        for trade in closed_trades:\n",
    "            # Return the invested capital plus profit/loss\n",
    "            self.available_capital += (trade.investment_black + trade.investment_white + trade.net_pnl)\n",
    "            # Add to trade history (only for closed trades)\n",
    "            self.trade_history.append(trade)\n",
    "            # Add to trade updates (for logging) - only adding CLOSED trades\n",
    "            trade_updates.append(trade.to_dict())\n",
    "        \n",
    "        # Calculate daily PnL from closed trades only\n",
    "        day_pnl = sum([trade.net_pnl for trade in closed_trades])\n",
    "        \n",
    "        # Then process new entries if we have signals and available capital\n",
    "        new_trades = self._process_entries(current_date, signals, current_data)\n",
    "        \n",
    "        # Update equity curve for accounting purposes\n",
    "        prev_equity = max(self.equity_curve.values())\n",
    "        self.equity_curve[current_date] = prev_equity + day_pnl\n",
    "        \n",
    "        # Save daily PnL\n",
    "        self.daily_pnl[current_date] = day_pnl\n",
    "        \n",
    "        # Return only the updates for CLOSED trades\n",
    "        return trade_updates\n",
    "    \n",
    "    def _check_exit_conditions(self, trade, current_date, current_z_diff):\n",
    "        \"\"\"Check if a trade should be exited based on the specified conditions\"\"\"\n",
    "        # Condition 1: Z-score mean reversion toward zero\n",
    "        if trade.side == 'short_black_long_white' and current_z_diff <= 0:\n",
    "            return True, 'mean_reversion'\n",
    "        elif trade.side == 'long_black_short_white' and current_z_diff >= 0:\n",
    "            return True, 'mean_reversion'\n",
    "            \n",
    "        # Condition 2: Max holding period reached\n",
    "        if trade.days_held >= self.max_holding_days:\n",
    "            return True, 'max_holding'\n",
    "            \n",
    "        return False, None\n",
    "    \n",
    "    def _process_exits(self, current_date, current_data):\n",
    "        \"\"\"Check active trades for exit conditions\"\"\"\n",
    "        closed_trades = []\n",
    "        remaining_trades = []\n",
    "        \n",
    "        for trade in self.active_trades:\n",
    "            # Get stock permnos\n",
    "            permno_black = trade.permno_black\n",
    "            permno_white = trade.permno_white\n",
    "            \n",
    "            # Get z-scores efficiently\n",
    "            z_col = f\"z_{trade.zscore_method}_{trade.horizon}d_lb{trade.lookback}\"\n",
    "            \n",
    "            # Check if data exists for both stocks\n",
    "            black_data = current_data[current_data['permno'] == permno_black]\n",
    "            white_data = current_data[current_data['permno'] == permno_white]\n",
    "            \n",
    "            if black_data.empty or white_data.empty or z_col not in black_data.columns or z_col not in white_data.columns:\n",
    "                remaining_trades.append(trade)\n",
    "                continue\n",
    "                \n",
    "            # Get current z-scores\n",
    "            z_black = black_data[z_col].values[0]\n",
    "            z_white = white_data[z_col].values[0]\n",
    "            \n",
    "            # Calculate current z-diff\n",
    "            current_z_diff = z_black - z_white\n",
    "            \n",
    "            # Check exit conditions\n",
    "            should_exit, exit_reason = self._check_exit_conditions(trade, current_date, current_z_diff)\n",
    "            \n",
    "            if should_exit:\n",
    "                # Get exit prices\n",
    "                exit_price_black = self.price_lookup.get((current_date, permno_black))\n",
    "                exit_price_white = self.price_lookup.get((current_date, permno_white))\n",
    "                \n",
    "                if exit_price_black is None or exit_price_white is None:\n",
    "                    # Can't exit if prices are missing, keep the trade\n",
    "                    remaining_trades.append(trade)\n",
    "                    continue\n",
    "                \n",
    "                # Close the trade\n",
    "                trade.close_trade(current_date, exit_price_black, exit_price_white, \n",
    "                                 exit_reason, current_z_diff)\n",
    "                \n",
    "                closed_trades.append(trade)\n",
    "            else:\n",
    "                # Keep track of active trades\n",
    "                remaining_trades.append(trade)\n",
    "        \n",
    "        # Update active trades list\n",
    "        self.active_trades = remaining_trades\n",
    "        return closed_trades\n",
    "    \n",
    "    def _process_entries(self, current_date, signals, current_data):\n",
    "        \"\"\"Process new trade entries with liquidity constraints\"\"\"\n",
    "        if not signals or self.available_capital <= 0:\n",
    "            return []\n",
    "            \n",
    "        # Calculate volatility for each pair in signals for position sizing\n",
    "        pairs_volatility = {}\n",
    "        total_inv_vol = 0\n",
    "        \n",
    "        # Track rejection reasons\n",
    "        missing_volatility = 0\n",
    "        \n",
    "        for sig in signals:\n",
    "            permno_black = sig['permno_black']\n",
    "            permno_white = sig['permno_white']\n",
    "            \n",
    "            # Get GARCH volatilities from lookup table\n",
    "            vol_black = self.volatility_lookup.get((current_date, permno_black))\n",
    "            vol_white = self.volatility_lookup.get((current_date, permno_white))\n",
    "            \n",
    "            if vol_black is None or vol_white is None or vol_black == 0 or vol_white == 0:\n",
    "                missing_volatility += 1\n",
    "                continue\n",
    "                \n",
    "            # Use combined volatility for the pair\n",
    "            pair_vol = (vol_black + vol_white) / 2\n",
    "            pair_key = (permno_black, permno_white)\n",
    "            pairs_volatility[pair_key] = pair_vol\n",
    "            \n",
    "            # Calculate inverse volatility\n",
    "            inv_vol = 1 / pair_vol\n",
    "            total_inv_vol += inv_vol\n",
    "        \n",
    "        if total_inv_vol == 0:\n",
    "            return []\n",
    "            \n",
    "        # Allocate capital by inverse volatility\n",
    "        capital_allocations = {}\n",
    "        for pair_key, vol in pairs_volatility.items():\n",
    "            inv_vol = 1 / vol\n",
    "            allocation = (inv_vol / total_inv_vol) * self.available_capital\n",
    "            capital_allocations[pair_key] = allocation\n",
    "        \n",
    "        # Execute trades\n",
    "        executed_trades = []\n",
    "        capital_used = 0\n",
    "        \n",
    "        # Track rejection reasons\n",
    "        missing_price = 0\n",
    "        zero_shares = 0\n",
    "        insufficient_capital = 0\n",
    "        \n",
    "        for sig in signals:\n",
    "            permno_black = sig['permno_black']\n",
    "            permno_white = sig['permno_white']\n",
    "            pair_key = (permno_black, permno_white)\n",
    "            \n",
    "            if pair_key not in capital_allocations:\n",
    "                continue\n",
    "                \n",
    "            # Get stock prices\n",
    "            px_b = self.price_lookup.get((current_date, permno_black))\n",
    "            px_w = self.price_lookup.get((current_date, permno_white))\n",
    "            \n",
    "            if px_b is None or px_w is None or px_b <= 0 or px_w <= 0:\n",
    "                missing_price += 1\n",
    "                continue\n",
    "                \n",
    "            # Allocate capital to the pair\n",
    "            pair_capital = capital_allocations[pair_key]\n",
    "            \n",
    "            # Split capital equally between black and white stocks\n",
    "            inv_b = inv_w = pair_capital / 2\n",
    "            \n",
    "            # Calculate maximum shares based on liquidity constraint (10% of ADV20)\n",
    "            max_shares_b = self._calculate_max_shares(permno_black, current_date, px_b, inv_b)\n",
    "            max_shares_w = self._calculate_max_shares(permno_white, current_date, px_w, inv_w)\n",
    "            \n",
    "            # Calculate shares based on capital allocation\n",
    "            capital_shares_b = int(inv_b / px_b)\n",
    "            capital_shares_w = int(inv_w / px_w)\n",
    "            \n",
    "            # Apply liquidity constraint\n",
    "            sh_b = min(capital_shares_b, max_shares_b) if max_shares_b > 0 else capital_shares_b\n",
    "            sh_w = min(capital_shares_w, max_shares_w) if max_shares_w > 0 else capital_shares_w\n",
    "            \n",
    "            # Skip if not enough shares can be purchased\n",
    "            if sh_b == 0 or sh_w == 0:\n",
    "                zero_shares += 1\n",
    "                continue\n",
    "                \n",
    "            # Recalculate actual investment based on constrained shares\n",
    "            inv_b = sh_b * px_b\n",
    "            inv_w = sh_w * px_w\n",
    "            \n",
    "            # Calculate transaction costs\n",
    "            entry_tc = 0.01 * (sh_b + sh_w)\n",
    "            \n",
    "            # Check if we have enough capital\n",
    "            total_cost = inv_b + inv_w + entry_tc\n",
    "            if total_cost > (self.available_capital - capital_used):\n",
    "                insufficient_capital += 1\n",
    "                continue\n",
    "                \n",
    "            # Record the capital used\n",
    "            capital_used += total_cost\n",
    "            \n",
    "            # Create new trade\n",
    "            new_trade = Trade(\n",
    "                entry_date=current_date,\n",
    "                permno_black=permno_black,\n",
    "                permno_white=permno_white,\n",
    "                side=sig['signal'],\n",
    "                z_diff_entry=sig['z_diff'],\n",
    "                investment_black=inv_b,\n",
    "                investment_white=inv_w,\n",
    "                shares_black=sh_b,\n",
    "                shares_white=sh_w,\n",
    "                entry_price_black=px_b,\n",
    "                entry_price_white=px_w,\n",
    "                entry_transaction_cost=entry_tc,\n",
    "                zscore_method=sig.get('zscore_method', 'ou'),\n",
    "                horizon=sig.get('horizon', 5),\n",
    "                lookback=sig.get('lookback', 20)\n",
    "            )\n",
    "            \n",
    "            # Add to active trades list\n",
    "            self.active_trades.append(new_trade)\n",
    "            executed_trades.append(new_trade)\n",
    "        \n",
    "        # Update available capital\n",
    "        self.available_capital -= capital_used\n",
    "        \n",
    "        return executed_trades\n",
    "\n",
    "    def mark_to_market_open_positions(self, final_date):\n",
    "        \"\"\"Close all open positions at the end of the backtest period using latest prices\"\"\"\n",
    "        closed_trades = []\n",
    "        \n",
    "        # Skip if no active trades\n",
    "        if not self.active_trades:\n",
    "            return []\n",
    "        \n",
    "        for trade in self.active_trades:\n",
    "            # Get exit prices for the final date\n",
    "            exit_price_black = self.price_lookup.get((final_date, trade.permno_black))\n",
    "            exit_price_white = self.price_lookup.get((final_date, trade.permno_white))\n",
    "            \n",
    "            # Skip if prices are missing\n",
    "            if exit_price_black is None or exit_price_white is None:\n",
    "                # Try to find the last available prices\n",
    "                dates = sorted(self.price_lookup.keys(), key=lambda x: x[0])\n",
    "                for date, permno in reversed(dates):\n",
    "                    if date < final_date and permno == trade.permno_black:\n",
    "                        exit_price_black = self.price_lookup.get((date, permno))\n",
    "                        break\n",
    "                \n",
    "                for date, permno in reversed(dates):\n",
    "                    if date < final_date and permno == trade.permno_white:\n",
    "                        exit_price_white = self.price_lookup.get((date, permno))\n",
    "                        break\n",
    "                \n",
    "                # If still no prices, skip this trade\n",
    "                if exit_price_black is None or exit_price_white is None:\n",
    "                    continue\n",
    "            \n",
    "            # Close the trade with \"end_of_period\" as reason\n",
    "            trade.close_trade(final_date, exit_price_black, exit_price_white, \n",
    "                             'end_of_period', 0)  # Use 0 as z_diff_exit\n",
    "            \n",
    "            # Add to closed trades and trade history\n",
    "            closed_trades.append(trade)\n",
    "            self.trade_history.append(trade)\n",
    "        \n",
    "        # Update active trades list (should be empty now)\n",
    "        self.active_trades = []\n",
    "        \n",
    "        # Update equity curve with the PnL from these trades\n",
    "        if closed_trades:\n",
    "            day_pnl = sum([trade.net_pnl for trade in closed_trades])\n",
    "            if final_date not in self.equity_curve:\n",
    "                prev_equity = max(self.equity_curve.values())\n",
    "                self.equity_curve[final_date] = prev_equity + day_pnl\n",
    "            else:\n",
    "                self.equity_curve[final_date] += day_pnl\n",
    "        \n",
    "        # Return the closed trades\n",
    "        return closed_trades\n",
    "        \n",
    "    def get_trade_history(self):\n",
    "        \"\"\"Return the trade history for analysis\"\"\"\n",
    "        return self.trade_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc98b23-0e8a-403f-bfd4-3392044206cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_trade_based_metrics(trade_df, market_returns, ffr_lookup=None, initial_capital=1_000_000_000):\n",
    "    \"\"\"\n",
    "    Calculate performance metrics based solely on trade log data and returns daily returns data\n",
    "    for graphing in reports.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    trade_df : pandas DataFrame\n",
    "        DataFrame containing trade information with columns: \n",
    "        entry_date, exit_date, net_pnl, etc.\n",
    "    market_returns : dict or Series\n",
    "        Market returns indexed by date (vwretd values)\n",
    "    ffr_lookup : dict or None\n",
    "        Federal Funds Rate lookup by date. If None, will use 0.02 as default.\n",
    "    initial_capital : float\n",
    "        Initial capital for calculating returns\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary of performance metrics and daily returns data\n",
    "    \"\"\"\n",
    "    # Ensure we have trades to analyze\n",
    "    if len(trade_df) == 0:\n",
    "        empty_returns = pd.DataFrame(columns=['date', 'return'])\n",
    "        return {\n",
    "            'sharpe_ratio': 0,\n",
    "            'sortino_ratio': 0,\n",
    "            'alpha': 0,\n",
    "            'beta': 0,\n",
    "            'max_drawdown': 0,\n",
    "            'hit_rate': 0,\n",
    "            'num_trades': 0,\n",
    "            'avg_trade_pnl': 0,\n",
    "            'avg_holding_period': 0,\n",
    "            'num_trading_days': 0,\n",
    "            'daily_returns': empty_returns\n",
    "        }\n",
    "    \n",
    "    # Convert dates to datetime if they aren't already\n",
    "    if not pd.api.types.is_datetime64_dtype(trade_df['exit_date']):\n",
    "        trade_df['exit_date'] = pd.to_datetime(trade_df['exit_date'])\n",
    "    if not pd.api.types.is_datetime64_dtype(trade_df['entry_date']):\n",
    "        trade_df['entry_date'] = pd.to_datetime(trade_df['entry_date'])\n",
    "    \n",
    "    # Sort trades by exit date\n",
    "    trade_df = trade_df.sort_values('exit_date')\n",
    "    \n",
    "    # Calculate basic trade metrics\n",
    "    num_trades = len(trade_df)\n",
    "    hit_rate = (trade_df['net_pnl'] > 0).mean()\n",
    "    avg_trade_pnl = trade_df['net_pnl'].mean()\n",
    "    avg_holding = trade_df['days_held'].mean() if 'days_held' in trade_df.columns else 0\n",
    "    \n",
    "    # Get unique trading days (both entry and exit dates)\n",
    "    trading_days = sorted(set(trade_df['entry_date'].tolist() + \n",
    "                             trade_df['exit_date'].dropna().tolist()))\n",
    "    num_trading_days = len(trading_days)\n",
    "    \n",
    "    # Calculate equity curve for performance metrics\n",
    "    equity_curve = {}\n",
    "    current_equity = initial_capital\n",
    "    \n",
    "    # Group trades by exit date and calculate daily PnL\n",
    "    for date, group in trade_df.groupby('exit_date'):\n",
    "        day_pnl = group['net_pnl'].sum()\n",
    "        current_equity += day_pnl\n",
    "        equity_curve[date] = current_equity\n",
    "    \n",
    "    # Convert to Series for easier manipulation\n",
    "    equity_series = pd.Series(equity_curve)\n",
    "    equity_series = equity_series.sort_index()\n",
    "    \n",
    "    # Handle case with insufficient data points\n",
    "    if len(equity_series) <= 1:\n",
    "        empty_returns = pd.DataFrame(columns=['date', 'return'])\n",
    "        return {\n",
    "            'sharpe_ratio': 0,\n",
    "            'sortino_ratio': 0,\n",
    "            'alpha': 0,\n",
    "            'beta': 0,\n",
    "            'max_drawdown': 0,\n",
    "            'hit_rate': hit_rate,\n",
    "            'num_trades': num_trades,\n",
    "            'avg_trade_pnl': avg_trade_pnl,\n",
    "            'avg_holding_period': avg_holding,\n",
    "            'num_trading_days': num_trading_days,\n",
    "            'daily_returns': empty_returns\n",
    "        }\n",
    "    \n",
    "    # Calculate daily returns\n",
    "    daily_returns = equity_series.pct_change().fillna(0)\n",
    "    \n",
    "    # Create a DataFrame of dates and returns for graphing\n",
    "    returns_df = pd.DataFrame({\n",
    "        'date': daily_returns.index,\n",
    "        'return': daily_returns.values\n",
    "    })\n",
    "    \n",
    "    # Get corresponding market returns\n",
    "    if isinstance(market_returns, dict):\n",
    "        market_returns_series = pd.Series({date: market_returns.get(date, 0) \n",
    "                                        for date in daily_returns.index})\n",
    "    else:\n",
    "        # Assume it's already a Series\n",
    "        market_returns_series = market_returns.loc[daily_returns.index]\n",
    "    \n",
    "    # Calculate average risk-free rate from Fed Funds Rate data if available\n",
    "    if ffr_lookup is not None:\n",
    "        # Extract Fed Funds Rates for trading days\n",
    "        trading_day_rates = [ffr_lookup.get(date, 0) for date in trading_days if date in ffr_lookup]\n",
    "        \n",
    "        # Calculate average Fed Funds Rate during trading period\n",
    "        if trading_day_rates:\n",
    "            avg_ffr = sum(trading_day_rates) / len(trading_day_rates)\n",
    "            # Convert annual rate to daily rate based on trading days\n",
    "            daily_rfr = avg_ffr / num_trading_days\n",
    "        else:\n",
    "            # Default to 2% if no rates found\n",
    "            daily_rfr = 0.02 / num_trading_days\n",
    "    else:\n",
    "        # Default to 2% if no FFR lookup provided\n",
    "        daily_rfr = 0.02 / num_trading_days\n",
    "    \n",
    "    # Calculate metrics based on daily returns\n",
    "    mean_daily_return = daily_returns.mean()\n",
    "    std_daily_return = daily_returns.std()\n",
    "    \n",
    "    # Calculate Sharpe ratio (based on daily returns)\n",
    "    sharpe_ratio = 0\n",
    "    if std_daily_return > 0:\n",
    "        sharpe_ratio = mean_daily_return / std_daily_return * np.sqrt(len(daily_returns))\n",
    "    \n",
    "    # Calculate Sortino ratio (based on daily returns)\n",
    "    downside_daily_returns = daily_returns[daily_returns < 0]\n",
    "    downside_std_daily = downside_daily_returns.std() if len(downside_daily_returns) > 0 else 0\n",
    "    sortino_ratio = 0\n",
    "    if downside_std_daily > 0:\n",
    "        sortino_ratio = mean_daily_return / downside_std_daily * np.sqrt(len(daily_returns))\n",
    "    \n",
    "    # Calculate CAPM metrics (Beta, Alpha)\n",
    "    beta = 0\n",
    "    alpha = 0\n",
    "    if len(daily_returns) > 1 and len(market_returns_series) > 1:\n",
    "        # Calculate beta\n",
    "        cov = np.cov(daily_returns, market_returns_series)[0, 1]\n",
    "        var = np.var(market_returns_series)\n",
    "        if var > 0:\n",
    "            beta = cov / var\n",
    "            \n",
    "            # Calculate alpha (based on actual trading days)\n",
    "            expected_return = daily_rfr + beta * (market_returns_series.mean() - daily_rfr)\n",
    "            alpha = (daily_returns.mean() - expected_return) * len(daily_returns)\n",
    "    \n",
    "    # Calculate drawdowns\n",
    "    peak = equity_series.expanding().max()\n",
    "    drawdowns = (equity_series - peak) / peak\n",
    "    max_drawdown = abs(drawdowns.min()) if len(drawdowns) > 0 else 0\n",
    "    \n",
    "    # Return comprehensive metrics and daily returns data\n",
    "    return {\n",
    "        'sharpe_ratio': sharpe_ratio,\n",
    "        'sortino_ratio': sortino_ratio,\n",
    "        'alpha': alpha,\n",
    "        'beta': beta,\n",
    "        'max_drawdown': max_drawdown,\n",
    "        'hit_rate': hit_rate,\n",
    "        'num_trades': num_trades,\n",
    "        'avg_trade_pnl': avg_trade_pnl,\n",
    "        'avg_holding_period': avg_holding,\n",
    "        'num_trading_days': num_trading_days,\n",
    "        'daily_returns': returns_df,\n",
    "        'avg_fed_funds_rate': avg_ffr if ffr_lookup is not None else 0.02  # Include average FFR in the results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0641ba5b-5ec0-4084-bf5e-1adadbe34bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "#from .signal_generator import SignalGenerator\n",
    "#from .portfolio_manager import PortfolioManager\n",
    "#from .performance import calculate_trade_based_metrics\n",
    "\n",
    "def _process_quarter_parallel(quarter, df_main, filtered_pairs, signal_generator, initial_capital, max_holding_days):\n",
    "    \"\"\"Process a single quarter in parallel\"\"\"\n",
    "    # Filter data for this quarter\n",
    "    quarter_data = df_main[df_main['quarter'] == quarter]\n",
    "    if quarter_data.empty:\n",
    "        print(f\"Warning: No data found for quarter {quarter}\")\n",
    "        return {'quarter': quarter, 'trade_log': [], 'performance': {}}\n",
    "    \n",
    "    print(f\"Processing calendar quarter {quarter}\")\n",
    "    \n",
    "    # Extract quarter start and end dates for better reporting\n",
    "    quarter_dates = sorted(quarter_data['date'].unique())\n",
    "    quarter_start = quarter_dates[0]\n",
    "    quarter_end = quarter_dates[-1]\n",
    "    print(f\"Quarter period: {quarter_start.strftime('%Y-%m-%d')} to {quarter_end.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    # Create portfolio manager for this quarter\n",
    "    portfolio_manager = PortfolioManager(\n",
    "        quarter_data, \n",
    "        initial_capital,\n",
    "        max_holding_days=max_holding_days\n",
    "    )\n",
    "    \n",
    "    # Reset capital\n",
    "    portfolio_manager.reset_capital(initial_capital)\n",
    "    \n",
    "    # Group data by date for faster access\n",
    "    date_grouped_data = {date: group for date, group in quarter_data.groupby('date')}\n",
    "    \n",
    "    # Process each trading day\n",
    "    trade_log = []\n",
    "    signals_count = 0\n",
    "    trades_count = 0\n",
    "    \n",
    "    # Process each trading day in the quarter\n",
    "    for current_date in quarter_dates:\n",
    "        # Get signals for the current date\n",
    "        try:\n",
    "            signals = signal_generator.generate_signals(current_date)\n",
    "            signals_count += len(signals)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating signals for date {current_date}: {str(e)}\")\n",
    "            signals = []\n",
    "        \n",
    "        # Process the trading day\n",
    "        try:\n",
    "            day_results = portfolio_manager.process_trading_day(\n",
    "                current_date, \n",
    "                signals,\n",
    "                date_grouped_data[current_date]\n",
    "            )\n",
    "            \n",
    "            if day_results:\n",
    "                trade_log.extend(day_results)\n",
    "                trades_count += len(day_results)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing trading day {current_date}: {str(e)}\")\n",
    "    \n",
    "    print(f\"Quarter {quarter} summary: {signals_count} signals generated, {trades_count} trades executed\")\n",
    "    \n",
    "    # Return the trade log for this quarter\n",
    "    return {\n",
    "        'quarter': quarter,\n",
    "        'trade_log': trade_log,\n",
    "        'performance': {}  # We'll calculate this later from the trade log\n",
    "    }\n",
    "    \n",
    "class BacktestEngine:\n",
    "    def __init__(self, df_main, df_pairs, hyperparams):\n",
    "        self.hyperparams = hyperparams\n",
    "        self.initial_capital = hyperparams['INITIAL_CAPITAL']\n",
    "        \n",
    "        # Select specific columns directly instead of filtering\n",
    "        zscore_method = hyperparams['ZSCORE_METHOD']\n",
    "        lookback_period = hyperparams['LOOKBACK_PERIOD']\n",
    "        horizon = hyperparams['HORIZON']\n",
    "        \n",
    "        z_col = f'z_{zscore_method}_{horizon}d_lb{lookback_period}'\n",
    "        base_cols = ['date', 'permno', 'trading_start', 'group_id', 'adj_prc', 'fed_funds_rate', 'adv20', 'vwretd', 'garch_vol']\n",
    "        future_return_col = f'future_cumret_{horizon}d'\n",
    "        \n",
    "        # Select only required columns\n",
    "        needed_cols = base_cols + [z_col]\n",
    "        if future_return_col in df_main.columns:\n",
    "            needed_cols.append(future_return_col)\n",
    "            \n",
    "        needed_cols = [col for col in needed_cols if col in df_main.columns]\n",
    "        \n",
    "        # Clean data\n",
    "        self.df_main = df_main[needed_cols].copy()\n",
    "        self.df_main = self.df_main.replace([np.inf, -np.inf], np.nan)\n",
    "        self.df_main = self.df_main.dropna()\n",
    "        \n",
    "        # Keep a copy of the pairs data\n",
    "        self.df_pairs = df_pairs\n",
    "        \n",
    "        # Pre-process data for faster lookups\n",
    "        self._preprocess_data()\n",
    "    \n",
    "    def _preprocess_data(self):\n",
    "        \"\"\"Preprocess data for efficient backtest execution\"\"\"\n",
    "        # Clean data by replacing infinites with NaN and dropping NaN values\n",
    "        self.df_main = self.df_main.replace([np.inf, -np.inf], np.nan)\n",
    "        self.df_main = self.df_main.dropna()\n",
    "        \n",
    "        # Make sure date is datetime\n",
    "        if not pd.api.types.is_datetime64_dtype(self.df_main['date']):\n",
    "            self.df_main['date'] = pd.to_datetime(self.df_main['date'])\n",
    "        \n",
    "        # Create a quarter column based on date\n",
    "        self.df_main['quarter'] = self.df_main['date'].dt.to_period('Q').astype(str)\n",
    "        \n",
    "        # Get unique quarters for processing\n",
    "        self.quarters = sorted(self.df_main['quarter'].unique())\n",
    "        \n",
    "        print(f\"Identified {len(self.quarters)} calendar quarters for processing\")\n",
    "        \n",
    "        # Filter pairs based on correlation and cointegration thresholds\n",
    "        corr_threshold = self.hyperparams['CORRELATION_THRESHOLD']\n",
    "        coint_threshold = self.hyperparams['COINTEGRATION_THRESHOLD']\n",
    "        \n",
    "        # Apply filters if thresholds are provided AND columns exist\n",
    "        filter_condition = True  # Default to include all pairs\n",
    "        \n",
    "        # Check correlation columns\n",
    "        if corr_threshold is not None:\n",
    "            if 'corr' in self.df_pairs.columns:\n",
    "                filter_condition = filter_condition & (self.df_pairs['corr'] >= corr_threshold)\n",
    "            elif 'correlation' in self.df_pairs.columns:\n",
    "                filter_condition = filter_condition & (self.df_pairs['correlation'] >= corr_threshold)\n",
    "            else:\n",
    "                print(\"Warning: No correlation column found in pairs data, skipping correlation filter\")\n",
    "        \n",
    "        # Check cointegration columns\n",
    "        if coint_threshold is not None:\n",
    "            if 'coint_pval' in self.df_pairs.columns:\n",
    "                filter_condition = filter_condition & (self.df_pairs['coint_pval'] <= coint_threshold)\n",
    "            elif 'pval' in self.df_pairs.columns:\n",
    "                filter_condition = filter_condition & (self.df_pairs['pval'] <= coint_threshold)\n",
    "            elif 'p_value' in self.df_pairs.columns:\n",
    "                filter_condition = filter_condition & (self.df_pairs['p_value'] <= coint_threshold)\n",
    "            else:\n",
    "                print(\"Warning: No cointegration p-value column found in pairs data, skipping cointegration filter\")\n",
    "        \n",
    "        # Apply the filter\n",
    "        self.filtered_pairs = self.df_pairs[filter_condition].copy()\n",
    "        \n",
    "        # Log preprocessing results\n",
    "        print(f\"Preprocessing complete: {len(self.filtered_pairs)} pairs after filtering\")\n",
    "    \n",
    "    def run_backtest(self):\n",
    "        \"\"\"Run the full backtest using the specified hyperparameters\"\"\"\n",
    "        print(\"Running pre-backtest diagnostics...\")\n",
    "        self.run_diagnostics()\n",
    "        \n",
    "        # Initialize signal generator with the selected parameters\n",
    "        zscore_method = self.hyperparams['ZSCORE_METHOD']\n",
    "        zscore_threshold = self.hyperparams['ZSCORE_THRESHOLD']\n",
    "        lookback_period = self.hyperparams['LOOKBACK_PERIOD']\n",
    "        horizon = self.hyperparams['HORIZON']\n",
    "        max_holding_days = self.hyperparams['MAX_HOLDING_DAYS']\n",
    "        \n",
    "        # Create optimized dataset with only needed columns\n",
    "        z_col = f'z_{zscore_method}_{horizon}d_lb{lookback_period}'\n",
    "        needed_cols = ['date', 'permno', 'quarter', 'group_id', 'adj_prc', 'fed_funds_rate', \n",
    "                      'adv20', 'vwretd', 'garch_vol', z_col]\n",
    "        \n",
    "        # Check if all needed columns exist\n",
    "        needed_cols = [col for col in needed_cols if col in self.df_main.columns]\n",
    "        \n",
    "        # Only keep needed columns in memory\n",
    "        self.optimized_df = self.df_main[needed_cols].copy()\n",
    "        \n",
    "        # Pre-sort data for faster operations\n",
    "        self.optimized_df.sort_values(['date', 'permno'], inplace=True)\n",
    "        \n",
    "        # Create signal generator with optimized dataset\n",
    "        signal_generator = SignalGenerator(\n",
    "            self.optimized_df, \n",
    "            self.filtered_pairs,\n",
    "            zscore_method=zscore_method,\n",
    "            zscore_threshold=zscore_threshold,\n",
    "            horizon=horizon,\n",
    "            lookback_period=lookback_period\n",
    "        )\n",
    "        \n",
    "        # Use fixed n_jobs=4 for parallel processing\n",
    "        n_jobs = 4\n",
    "        \n",
    "        # Precompute signals with progress bar\n",
    "        signal_generator.precompute_signals_parallel(horizon=horizon, n_jobs=n_jobs)\n",
    "        \n",
    "        # Check if we have any quarters to process\n",
    "        if len(self.quarters) == 0:\n",
    "            print(\"No quarters found to process! Check data filtering.\")\n",
    "            # Return empty results\n",
    "            return {\n",
    "                'trade_log': pd.DataFrame(),\n",
    "                'performance': {},\n",
    "                'hyperparams': self.hyperparams,\n",
    "                'quarterly_results': {}\n",
    "            }\n",
    "        \n",
    "        # Process quarters in batches to reduce memory pressure\n",
    "        batch_size = 4  # Adjust based on your system's memory\n",
    "        all_closed_trades = []\n",
    "        quarterly_results = {}\n",
    "        \n",
    "        for i in range(0, len(self.quarters), batch_size):\n",
    "            batch_quarters = self.quarters[i:i+batch_size]\n",
    "            \n",
    "            # Process quarters in parallel\n",
    "            n_jobs = 4\n",
    "            batch_results = Parallel(n_jobs=n_jobs, prefer=\"threads\")(\n",
    "                delayed(_process_quarter_parallel)(\n",
    "                    quarter,\n",
    "                    self.optimized_df,\n",
    "                    self.filtered_pairs,\n",
    "                    signal_generator,\n",
    "                    self.initial_capital,\n",
    "                    max_holding_days\n",
    "                )\n",
    "                for quarter in tqdm(batch_quarters, desc=f\"Processing Quarters Batch {i//batch_size+1}\")\n",
    "            )\n",
    "            \n",
    "            # Collect results\n",
    "            for result in batch_results:\n",
    "                all_closed_trades.extend(result['trade_log'])\n",
    "                quarterly_results[result['quarter']] = result['performance']\n",
    "            \n",
    "            # Force garbage collection after each batch\n",
    "            gc.collect()\n",
    "        \n",
    "        print(f\"\\nCollected {len(all_closed_trades)} closed trades across all quarters\")\n",
    "        \n",
    "        # Calculate performance metrics from trade log\n",
    "        performance_metrics = {}\n",
    "        \n",
    "        if len(all_closed_trades) > 0:\n",
    "            # Convert trade_log to DataFrame for analysis\n",
    "            trade_df = pd.DataFrame(all_closed_trades) if all_closed_trades else pd.DataFrame()\n",
    "            \n",
    "            # Calculate metrics directly from trades\n",
    "            if len(trade_df) > 0:\n",
    "                # Create lookups for market returns and Fed Funds Rate from the original data\n",
    "                date_indexed = self.df_main.drop_duplicates('date').set_index('date')\n",
    "                market_return_lookup = date_indexed['vwretd'].to_dict()\n",
    "                ffr_lookup = date_indexed['fed_funds_rate'].to_dict()\n",
    "                \n",
    "                # Calculate comprehensive metrics directly from trade log\n",
    "                performance_metrics = calculate_trade_based_metrics(\n",
    "                    trade_df=trade_df,\n",
    "                    market_returns=market_return_lookup,\n",
    "                    ffr_lookup=ffr_lookup,\n",
    "                    initial_capital=self.initial_capital\n",
    "                )\n",
    "                \n",
    "                # Save daily returns data to file for graphing\n",
    "                if 'daily_returns' in performance_metrics:\n",
    "                    daily_returns_df = performance_metrics['daily_returns']\n",
    "                    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "                    daily_returns_file = f'daily_returns_{timestamp}.csv'\n",
    "                    daily_returns_df.to_csv(daily_returns_file, index=False)\n",
    "                    print(f\"Daily returns data saved to {daily_returns_file}\")\n",
    "                \n",
    "                # Print metrics\n",
    "                print(f\"\\nCalculated metrics from trade data:\")\n",
    "                print(f\"Number of trades: {performance_metrics['num_trades']}\")\n",
    "                print(f\"Number of unique trading days: {performance_metrics['num_trading_days']}\")\n",
    "                print(f\"Average Fed Funds Rate: {performance_metrics['avg_fed_funds_rate']*100:.2f}%\")\n",
    "                print(f\"Hit rate: {performance_metrics['hit_rate']*100:.2f}%\")\n",
    "                print(f\"Average trade PnL: ${performance_metrics['avg_trade_pnl']:,.2f}\")\n",
    "                print(f\"Average holding period: {performance_metrics['avg_holding_period']:.2f} days\")\n",
    "                print(f\"Sharpe ratio (based on trading days): {performance_metrics['sharpe_ratio']:.4f}\")\n",
    "                print(f\"Sortino ratio (based on trading days): {performance_metrics['sortino_ratio']:.4f}\")\n",
    "                print(f\"CAPM Alpha: {performance_metrics['alpha']:.6f}\")\n",
    "                print(f\"CAPM Beta: {performance_metrics['beta']:.4f}\")\n",
    "                print(f\"Max drawdown: {performance_metrics['max_drawdown']*100:.2f}%\")\n",
    "        else:\n",
    "            print(\"No closed trades found, using empty metrics\")\n",
    "            \n",
    "        # Return combined results\n",
    "        results = {\n",
    "            'trade_log': pd.DataFrame(all_closed_trades) if all_closed_trades else pd.DataFrame(),\n",
    "            'performance': performance_metrics,\n",
    "            'hyperparams': self.hyperparams,\n",
    "            'quarterly_results': quarterly_results\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    def run_diagnostics(self):\n",
    "        \"\"\"Run diagnostic checks to identify potential issues\"\"\"\n",
    "        print(\"\\n=== DIAGNOSTIC REPORT ===\\n\")\n",
    "        \n",
    "        # 1. Check for pairs after filtering\n",
    "        if hasattr(self, \"filtered_pairs\"):\n",
    "            print(f\"Filtered pairs: {len(self.filtered_pairs)} of {len(self.df_pairs)} original pairs\")\n",
    "            if len(self.filtered_pairs) == 0:\n",
    "                print(\"CRITICAL ERROR: No pairs remain after correlation/cointegration filtering!\")\n",
    "                \n",
    "                # Check correlation threshold\n",
    "                corr_threshold = self.hyperparams.get('CORRELATION_THRESHOLD')\n",
    "                if corr_threshold is not None:\n",
    "                    for corr_col in ['corr', 'correlation']:\n",
    "                        if corr_col in self.df_pairs.columns:\n",
    "                            corr_values = self.df_pairs[corr_col].dropna()\n",
    "                            print(f\"{corr_col} stats: min={corr_values.min():.4f}, max={corr_values.max():.4f}, mean={corr_values.mean():.4f}\")\n",
    "                            above_threshold = (corr_values >= corr_threshold).sum()\n",
    "                            print(f\"Values >= {corr_threshold}: {above_threshold} ({above_threshold/len(corr_values)*100:.2f}%)\")\n",
    "                            break\n",
    "                \n",
    "                # Check cointegration threshold\n",
    "                coint_threshold = self.hyperparams.get('COINTEGRATION_THRESHOLD')\n",
    "                if coint_threshold is not None:\n",
    "                    for coint_col in ['coint_pval', 'pval', 'p_value']:\n",
    "                        if coint_col in self.df_pairs.columns:\n",
    "                            coint_values = self.df_pairs[coint_col].dropna()\n",
    "                            print(f\"{coint_col} stats: min={coint_values.min():.4f}, max={coint_values.max():.4f}, mean={coint_values.mean():.4f}\")\n",
    "                            below_threshold = (coint_values <= coint_threshold).sum()\n",
    "                            print(f\"Values <= {coint_threshold}: {below_threshold} ({below_threshold/len(coint_values)*100:.2f}%)\")\n",
    "                            break\n",
    "        \n",
    "        # 2. Check for z-score columns\n",
    "        zscore_method = self.hyperparams['ZSCORE_METHOD']\n",
    "        lookback_period = self.hyperparams['LOOKBACK_PERIOD']\n",
    "        horizon = self.hyperparams['HORIZON']\n",
    "        \n",
    "        z_col = f'z_{zscore_method}_{horizon}d_lb{lookback_period}'\n",
    "        print(f\"\\nChecking for z-score column: {z_col}\")\n",
    "        \n",
    "        if z_col in self.df_main.columns:\n",
    "            z_values = self.df_main[z_col].dropna()\n",
    "            z_threshold = self.hyperparams['ZSCORE_THRESHOLD']\n",
    "            \n",
    "            print(f\"Z-score column stats:\")\n",
    "            print(f\"  - Non-null values: {len(z_values)} out of {len(self.df_main)} ({len(z_values)/len(self.df_main)*100:.2f}%)\")\n",
    "            print(f\"  - Range: {z_values.min():.4f} to {z_values.max():.4f}\")\n",
    "            print(f\"  - Values exceeding threshold {z_threshold}: {(abs(z_values) >= z_threshold).sum()} ({(abs(z_values) >= z_threshold).sum()/len(z_values)*100:.2f}%)\")\n",
    "        else:\n",
    "            print(f\"CRITICAL ERROR: Z-score column '{z_col}' not found in data!\")\n",
    "            z_cols = [col for col in self.df_main.columns if col.startswith('z_')]\n",
    "            if z_cols:\n",
    "                print(f\"Available z-score columns: {z_cols}\")\n",
    "            else:\n",
    "                print(\"No z-score columns found in data!\")\n",
    "        \n",
    "        # 3. Check for essential columns\n",
    "        required_cols = ['date', 'permno', 'group_id', 'adj_prc', 'fed_funds_rate', 'adv20', 'vwretd', 'garch_vol']\n",
    "        missing_cols = [col for col in required_cols if col not in self.df_main.columns]\n",
    "        \n",
    "        if missing_cols:\n",
    "            print(f\"\\nMISSING REQUIRED COLUMNS: {missing_cols}\")\n",
    "        else:\n",
    "            print(\"\\nAll required base columns are present\")\n",
    "        \n",
    "        # 4. Check for NaN values in essential columns\n",
    "        print(\"\\nNaN check for essential columns:\")\n",
    "        for col in required_cols:\n",
    "            if col in self.df_main.columns:\n",
    "                null_count = self.df_main[col].isna().sum()\n",
    "                null_pct = null_count / len(self.df_main) * 100\n",
    "                print(f\"  - {col}: {null_count} NaN values ({null_pct:.2f}%)\")\n",
    "        \n",
    "        print(\"\\n=== END OF DIAGNOSTIC REPORT ===\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cc2f1d-7072-4750-b91f-a50afaf50af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "\n",
    "#from .backtest_engine import BacktestEngine\n",
    "\n",
    "def run_hyperparameter_grid_search(df_main, df_pairs, param_grid, output_file='backtest_results.csv'):\n",
    "    \"\"\"Run backtest with different hyperparameter combinations\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Generate parameter combinations more efficiently\n",
    "    keys = list(param_grid.keys())\n",
    "    values = list(param_grid.values())\n",
    "    param_combinations = []\n",
    "    \n",
    "    # Get all parameter combinations except INITIAL_CAPITAL\n",
    "    non_capital_keys = [k for k in keys if k != 'INITIAL_CAPITAL']\n",
    "    non_capital_values = [param_grid[k] for k in non_capital_keys]\n",
    "    \n",
    "    # Generate combinations with product\n",
    "    for combination in product(*non_capital_values):\n",
    "        params = dict(zip(non_capital_keys, combination))\n",
    "        params['INITIAL_CAPITAL'] = param_grid['INITIAL_CAPITAL']\n",
    "        param_combinations.append(params)\n",
    "    \n",
    "    print(f\"Running {len(param_combinations)} parameter combinations\")\n",
    "    \n",
    "    # Use a checkpointing mechanism\n",
    "    checkpoint_file = f\"checkpoint_{os.path.basename(output_file)}.pkl\"\n",
    "    completed_runs = set()\n",
    "    try:\n",
    "        if os.path.exists(checkpoint_file):\n",
    "            with open(checkpoint_file, 'rb') as f:\n",
    "                checkpoint_data = pickle.load(f)\n",
    "                results = checkpoint_data.get('results', [])\n",
    "                completed_runs = set(checkpoint_data.get('completed', []))\n",
    "                print(f\"Loaded {len(results)} previous results from checkpoint\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading checkpoint: {str(e)}. Starting fresh.\")\n",
    "        results = []\n",
    "        completed_runs = set()\n",
    "    \n",
    "    # Run backtest for each combination\n",
    "    for i, params in enumerate(param_combinations):\n",
    "        # Skip already completed runs\n",
    "        params_str = str(params)\n",
    "        if params_str in completed_runs:\n",
    "            print(f\"Skipping combination {i+1}/{len(param_combinations)}: already completed\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Running combination {i+1}/{len(param_combinations)}: {params}\")\n",
    "        \n",
    "        try:\n",
    "            # Create a different random seed for each run for reproducibility\n",
    "            seed = hash(params_str) % 10000\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "            backtest = BacktestEngine(df_main, df_pairs, params)\n",
    "            result = backtest.run_backtest()\n",
    "            \n",
    "            # Extract performance metrics\n",
    "            performance = result['performance']\n",
    "\n",
    "            # Save trade log to file with timestamp\n",
    "            timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "            if not result['trade_log'].empty:\n",
    "                trade_log_file = f\"trade_log_{params['ZSCORE_METHOD']}_{params['ZSCORE_THRESHOLD']}_{params['LOOKBACK_PERIOD']}_{params['HORIZON']}_{params['MAX_HOLDING_DAYS']}_{timestamp}.csv\"\n",
    "                result['trade_log'].to_csv(trade_log_file, index=False)\n",
    "                print(f\"Saved {len(result['trade_log'])} trades to {trade_log_file}\")\n",
    "            else:\n",
    "                print(\"No trades to save!\")\n",
    "            \n",
    "            # Combine parameters and performance metrics for output\n",
    "            result_row = {\n",
    "                'CORRELATION_THRESHOLD': params['CORRELATION_THRESHOLD'],\n",
    "                'COINTEGRATION_THRESHOLD': params['COINTEGRATION_THRESHOLD'],\n",
    "                'ZSCORE_METHOD': params['ZSCORE_METHOD'],\n",
    "                'ZSCORE_THRESHOLD': params['ZSCORE_THRESHOLD'],\n",
    "                'LOOKBACK_PERIOD': params['LOOKBACK_PERIOD'],\n",
    "                'HORIZON': params['HORIZON'],\n",
    "                'MAX_HOLDING_DAYS': params['MAX_HOLDING_DAYS'],\n",
    "                'sharpe_ratio': performance.get('sharpe_ratio', 0),\n",
    "                'sortino_ratio': performance.get('sortino_ratio', 0),\n",
    "                'alpha': performance.get('alpha', 0),\n",
    "                'beta': performance.get('beta', 0),\n",
    "                'max_drawdown': performance.get('max_drawdown', 0),\n",
    "                'hit_rate': performance.get('hit_rate', 0),\n",
    "                'num_trades': performance.get('num_trades', 0),\n",
    "                'avg_trade_pnl': performance.get('avg_trade_pnl', 0),\n",
    "                'avg_holding_period': performance.get('avg_holding_period', 0),\n",
    "                'num_trading_days': performance.get('num_trading_days', 0)\n",
    "            }\n",
    "            \n",
    "            results.append(result_row)\n",
    "            completed_runs.add(params_str)\n",
    "            \n",
    "            # Save checkpoint after each successful run\n",
    "            try:\n",
    "                with open(checkpoint_file, 'wb') as f:\n",
    "                    pickle.dump({'results': results, 'completed': list(completed_runs)}, f)\n",
    "                \n",
    "                # Save to CSV as well\n",
    "                pd.DataFrame(results).to_csv(output_file, index=False)\n",
    "            except Exception as save_err:\n",
    "                print(f\"Error saving checkpoint: {str(save_err)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error running combination {i+1}: {params}\")\n",
    "            print(f\"Error details: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            \n",
    "            # Add a row with error information\n",
    "            error_row = {\n",
    "                'CORRELATION_THRESHOLD': params['CORRELATION_THRESHOLD'],\n",
    "                'COINTEGRATION_THRESHOLD': params['COINTEGRATION_THRESHOLD'],\n",
    "                'ZSCORE_METHOD': params['ZSCORE_METHOD'],\n",
    "                'ZSCORE_THRESHOLD': params['ZSCORE_THRESHOLD'],\n",
    "                'LOOKBACK_PERIOD': params['LOOKBACK_PERIOD'],\n",
    "                'HORIZON': params['HORIZON'],\n",
    "                'MAX_HOLDING_DAYS': params['MAX_HOLDING_DAYS'],\n",
    "                'error': str(e),\n",
    "                'sharpe_ratio': 0,\n",
    "                'sortino_ratio': 0,\n",
    "                'alpha': 0,\n",
    "                'beta': 0,\n",
    "                'max_drawdown': 0,\n",
    "                'hit_rate': 0,\n",
    "                'num_trades': 0,\n",
    "                'avg_trade_pnl': 0,\n",
    "                'avg_holding_period': 0,\n",
    "                'num_trading_days': 0\n",
    "            }\n",
    "            results.append(error_row)\n",
    "            \n",
    "            # Save checkpoint and CSV after error\n",
    "            try:\n",
    "                with open(checkpoint_file, 'wb') as f:\n",
    "                    pickle.dump({'results': results, 'completed': list(completed_runs)}, f)\n",
    "                pd.DataFrame(results).to_csv(output_file, index=False)\n",
    "            except Exception as save_err:\n",
    "                print(f\"Error saving checkpoint after error: {str(save_err)}\")\n",
    "    \n",
    "    # Final save and return\n",
    "    try:\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.to_csv(output_file, index=False)\n",
    "        return results_df\n",
    "    except Exception as final_err:\n",
    "        print(f\"Error saving final results: {str(final_err)}\")\n",
    "        return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0a0545-14f1-4087-965c-6cfd576b1fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "#from .grid_search import run_hyperparameter_grid_search  # Uncomment this import\n",
    "\n",
    "def run_backtest(df_main_path='final_backtest_data.csv', \n",
    "               df_pairs_path='corr_coin.csv',\n",
    "               period='train'):\n",
    "    \"\"\"Main function to run the backtest\"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    \n",
    "    try:\n",
    "        # Load the datasets with the correct filenames\n",
    "        try:\n",
    "            df_merged_filtered = pd.read_csv(df_main_path)\n",
    "            print(f\"Successfully loaded {df_main_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {df_main_path}: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "        try:\n",
    "            df_pairs = pd.read_csv(df_pairs_path)\n",
    "            print(f\"Successfully loaded {df_pairs_path}\")\n",
    "            print(f\"Columns in df_pairs: {list(df_pairs.columns)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {df_pairs_path}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        # Rename column names if needed\n",
    "        if 'permno_1' in df_pairs.columns and 'permno_2' in df_pairs.columns:\n",
    "            df_pairs.rename(columns={'permno_1': 'permno_black', 'permno_2': 'permno_white'}, inplace=True)\n",
    "        \n",
    "        # Convert date columns to datetime\n",
    "        df_merged_filtered['date'] = pd.to_datetime(df_merged_filtered['date'])\n",
    "        \n",
    "        # Filter data based on period\n",
    "        if period.lower() == 'train':\n",
    "            start_date = '2015-01-01'\n",
    "            end_date = '2021-12-31'\n",
    "            period_name = \"in-sample\"\n",
    "        elif period.lower() == 'test':\n",
    "            start_date = '2022-01-01'\n",
    "            end_date = '2024-12-31'\n",
    "            period_name = \"out-of-sample\"\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid period: {period}. Use 'train' or 'test'.\")\n",
    "        \n",
    "        # Filter main dataframe by date\n",
    "        date_mask = (df_merged_filtered['date'] >= start_date) & (df_merged_filtered['date'] <= end_date)\n",
    "        df_merged_filtered = df_merged_filtered[date_mask].copy()\n",
    "\n",
    "        # Define quarters based on calendar date\n",
    "        df_merged_filtered['quarter'] = df_merged_filtered['date'].dt.to_period('Q').astype(str)\n",
    "        \n",
    "        # Filter pairs by date range if formation_date exists\n",
    "        if 'formation_date' in df_pairs.columns:\n",
    "            df_pairs['formation_date'] = pd.to_datetime(df_pairs['formation_date'])\n",
    "            date_mask = (df_pairs['formation_date'] >= start_date) & (df_pairs['formation_date'] <= end_date)\n",
    "            df_pairs = df_pairs[date_mask].copy()\n",
    "            print(f\"Filtered pairs: {len(df_pairs)} within date range\")\n",
    "        \n",
    "        # Print data overview\n",
    "        quarters = df_merged_filtered['quarter'].unique()\n",
    "        print(f\"\\n=== Data overview for {period_name} period ({start_date} to {end_date}) ===\")\n",
    "        print(f\"Date range: {df_merged_filtered['date'].min()} to {df_merged_filtered['date'].max()}\")\n",
    "        print(f\"Number of trading days: {df_merged_filtered['date'].nunique()}\")\n",
    "        print(f\"Number of stocks: {df_merged_filtered['permno'].nunique()}\")\n",
    "        print(f\"Number of calendar quarters: {len(quarters)}\")\n",
    "        print(f\"Number of pairs: {len(df_pairs)}\")\n",
    "        \n",
    "        # Define hyperparameter grid\n",
    "        param_grid = {\n",
    "            'COINTEGRATION_THRESHOLD': [0.05],\n",
    "            'CORRELATION_THRESHOLD': [0.9], #0.5, 0.7, ],\n",
    "            'ZSCORE_METHOD': ['classical'], #'ou'],\n",
    "            'ZSCORE_THRESHOLD': [1],\n",
    "            'LOOKBACK_PERIOD': [10],\n",
    "            'HORIZON': [10],\n",
    "            'MAX_HOLDING_DAYS': [10],\n",
    "            'INITIAL_CAPITAL': 1_000_000_000\n",
    "        }\n",
    "        \n",
    "        # Output file path\n",
    "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_file = f'backtest_results_{period}_{timestamp}.csv'\n",
    "        \n",
    "        # Calculate number of combinations\n",
    "        num_combinations = 1\n",
    "        for key, values in param_grid.items():\n",
    "            if isinstance(values, list):\n",
    "                num_combinations *= len(values)\n",
    "        \n",
    "        print(f\"Starting grid search with {num_combinations} combinations...\")\n",
    "        \n",
    "        # Run grid search\n",
    "        results = run_hyperparameter_grid_search(df_merged_filtered, df_pairs, param_grid, output_file)\n",
    "        \n",
    "        # Print summary of best results\n",
    "        if not results.empty:\n",
    "            print(\"\\nTop 5 parameter combinations by Sharpe ratio:\")\n",
    "            top_sharpe = results.sort_values('sharpe_ratio', ascending=False).head(5)\n",
    "            print(top_sharpe[['ZSCORE_METHOD', 'ZSCORE_THRESHOLD', 'LOOKBACK_PERIOD', 'HORIZON', 'MAX_HOLDING_DAYS', 'sharpe_ratio', 'sortino_ratio', 'alpha']])\n",
    "            \n",
    "            # Save the best performing parameters for future use\n",
    "            try:\n",
    "                best_params_idx = results['sharpe_ratio'].idxmax()\n",
    "                best_params = results.loc[best_params_idx].to_dict()\n",
    "                with open(f'best_params_{period}_{timestamp}.txt', 'w') as f:\n",
    "                    for k, v in best_params.items():\n",
    "                        f.write(f\"{k}: {v}\\n\")\n",
    "                \n",
    "                print(f\"\\nResults saved to {output_file}\")\n",
    "                print(f\"Best parameters saved to best_params_{period}_{timestamp}.txt\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving best parameters: {str(e)}\")\n",
    "        else:\n",
    "            print(\"No valid results were generated. Check the error logs.\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e486b0-af73-458a-99a3-c537b31bfd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from .trade import Trade\n",
    "#from .signal_generator import SignalGenerator\n",
    "#from .portfolio_manager import PortfolioManager\n",
    "#from .performance import calculate_trade_based_metrics\n",
    "#from .backtest_engine import BacktestEngine\n",
    "#from .grid_search import run_hyperparameter_grid_search\n",
    "#from .main import run_backtest\n",
    "\n",
    "__all__ = [\n",
    "    'Trade',\n",
    "    'SignalGenerator',\n",
    "    'PortfolioManager',\n",
    "    'calculate_trade_based_metrics',\n",
    "    'BacktestEngine',\n",
    "    'run_hyperparameter_grid_search',\n",
    "    'run_backtest'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae611b4-4769-4ecb-8547-ba5e62584807",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Specify period as 'train' or 'test'\n",
    "    period = 'test'\n",
    "    \n",
    "    print(f\"Running backtest for period: {period}\")\n",
    "    \n",
    "    # Run the backtest\n",
    "    results = run_backtest(\n",
    "        df_main_path='final_backtest_data.csv',\n",
    "        df_pairs_path='corr_coin.csv',\n",
    "        period=period\n",
    "    )\n",
    "    \n",
    "    if results is not None:\n",
    "        print(\"Backtest completed successfully!\")\n",
    "    else:\n",
    "        print(\"Backtest failed. Check error logs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7eee20-27d4-4eea-9ecc-170097a007b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
