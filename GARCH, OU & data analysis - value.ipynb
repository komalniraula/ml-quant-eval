{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15823e65-67e1-47ec-a52f-b8d6136b0d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import datetime\n",
    "\n",
    "from arch import arch_model\n",
    "\n",
    "import numpy.linalg as la\n",
    "from pandas.tseries.offsets import BDay\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5df690-896c-41d9-ae8f-93113cff7547",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustered = pd.read_csv(\"knn-dataset_value.csv\")\n",
    "df_crsp = pd.read_csv('stock_daily.csv')\n",
    "df_crsp['date'] = pd.to_datetime(df_crsp['date'])\n",
    "df_crsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c0a960-cd5f-43a3-9416-54decdca8fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keeping the permno that exist in df_clustered \n",
    "# -------------------------------------------------------------\n",
    "# Create a reference key from df_clustered\n",
    "valid_keys = df_clustered[['permno', 'trading_start']].drop_duplicates()\n",
    "\n",
    "# Assign trading_start to df_crsp\n",
    "df_crsp['trading_start'] = df_crsp['date'].dt.to_period('Q').dt.start_time\n",
    "\n",
    "valid_keys['trading_start'] = pd.to_datetime(valid_keys['trading_start'])\n",
    "df_crsp['trading_start'] = pd.to_datetime(df_crsp['trading_start'])\n",
    "\n",
    "# Keep only CRSP rows for stocks in df_clustered for that trading quarter\n",
    "df_crsp_filtered = pd.merge(df_crsp, valid_keys, on=['permno', 'trading_start'], how='inner')\n",
    "df_crsp_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47905aa0-f529-4eb1-8527-3e9c8c611ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustered['trading_start'] = pd.to_datetime(df_clustered['trading_start'])\n",
    "\n",
    "df_merged = pd.merge(\n",
    "    df_crsp_filtered,\n",
    "    df_clustered[['permno', 'trading_start', 'group_id']],\n",
    "    on=['permno', 'trading_start'],\n",
    "    how='left'\n",
    ")\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa99b81-cde8-42e1-a237-82bc3aa683f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of unique stocks per group_id\n",
    "group_sizes = df_merged.groupby('group_id')['permno'].nunique().reset_index()\n",
    "group_sizes = group_sizes.rename(columns={'permno': 'group_size'})\n",
    "\n",
    "# Keep only groups with 10 or more stocks\n",
    "valid_groups = group_sizes[group_sizes['group_size'] >= 10]['group_id']\n",
    "\n",
    "# Filter df_merged to include only valid groups\n",
    "df_merged = df_merged[df_merged['group_id'].isin(valid_groups)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Remaining groups after filtering: {df_merged['group_id'].nunique()}\")\n",
    "print(f\"Remaining rows in df_merged: {len(df_merged)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e0e0bb-8ec6-4554-b6e0-46771c9850d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['vol'] = df_merged['vol'].fillna(0) / 100           # ‚Üê changed\n",
    "# rolling 20 day average volumne\n",
    "df_merged['adv20'] = (df_merged.groupby('permno')['vol']\n",
    "                      .rolling(20, min_periods=1).mean()\n",
    "                      .reset_index(level=0, drop=True))\n",
    "\n",
    "# adjusted price\n",
    "df_merged['adj_prc'] = df_merged['prc'] / df_merged['cfacpr']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96701f0-6524-4be5-9d5d-3ba2ec73a1d1",
   "metadata": {},
   "source": [
    "### Analysis and update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e1bbcf-f710-4f60-bb85-9e21826d8d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace inf and -inf with NaN\n",
    "df_merged['adj_prc'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Drop rows with NaN adjusted prices\n",
    "df_merged = df_merged.dropna(subset=['adj_prc'])\n",
    "\n",
    "df_merged['adj_prc'].describe() # confirm cleanup worked\n",
    "\n",
    "# Ensure No Negative Prices (Exclude Returns on Delisted Days)\n",
    "df_merged = df_merged[df_merged['adj_prc'] > 0]\n",
    "\n",
    "# sort values\n",
    "df_merged = df_merged.sort_values(by=['permno', 'date']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfdeb13-4dc6-4873-ba11-e54c8d3e527e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic stats\n",
    "df_merged['adj_prc'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96028f7e-54e7-4f74-956f-21190f4a348e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify tickers with data from BOTH 2000 and 2025\n",
    "tickers_with_full_span = (\n",
    "    df_merged.groupby('ticker')['date']\n",
    "    .agg(['min', 'max'])\n",
    "    .query(\"min <= '2000-01-01' and max >= '2024-12-30'\")\n",
    "    .index.tolist()\n",
    ")\n",
    "\n",
    "print(f\"Number of tickers with full span coverage: {len(tickers_with_full_span)}\")\n",
    "\n",
    "# Visualize Sample from These Tickers\n",
    "sample_tickers = tickers_with_full_span[2:5]  # Select first 5 for visualization\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for ticker in sample_tickers:\n",
    "    subset = df_merged[df_merged['ticker'] == ticker]\n",
    "    plt.plot(subset['date'], subset['adj_prc'], label=ticker)\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"üìà Price Trends for Stocks with Data from 2000 to 2025\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Adjusted Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45faab3b-0218-4bc0-93d6-2cce80b8d0ec",
   "metadata": {},
   "source": [
    "## üìö **Volatility Analysis ‚Äì Rolling vs. GARCH**\n",
    "\n",
    "Understanding volatility is critical in portfolio management and risk assessment.  \n",
    "In this section, we compare two approaches:\n",
    "\n",
    "1. üìè **Rolling (Historical) Volatility:**  \n",
    "   - Simple method that calculates standard deviation over a fixed time window (e.g., last 20 days).\n",
    "   - Assumes volatility changes slowly and doesn't account for sudden market shocks.\n",
    "\n",
    "2. üìà **GARCH (Generalized Autoregressive Conditional Heteroskedasticity):**  \n",
    "   - Advanced model that dynamically estimates volatility based on past volatility and past shocks.\n",
    "   - Captures **volatility clustering**, which is common in financial markets (periods of calm followed by high turbulence).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2e87a9-a129-4f40-b280-ecf80cdb9510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20-day rolling volatility\n",
    "df_merged['rolling_vol_20d_daily'] = (\n",
    "    df_merged.groupby('permno')['retx']\n",
    "    .rolling(window=20, min_periods=15) # looks at past 20 days returns to calculate today's volatility\n",
    "    .std()\n",
    "    .reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "\n",
    "# Calculate 20-Day Rolling Volatility (Annualized)\n",
    "df_merged['rolling_vol_20d'] = (\n",
    "    df_merged.groupby('permno')['retx']\n",
    "    .rolling(window=20, min_periods=15) # looks at past 20 days returns to calculate today's volatility\n",
    "    .std()\n",
    "    .reset_index(level=0, drop=True) * np.sqrt(252)  # Annualize\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b22d6ff-876b-410f-80c0-e927de551f12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculate_garch_vol(series):\n",
    "    if series.dropna().shape[0] < 100:\n",
    "        return pd.Series(index=series.index, data=np.nan) # if there's not enough data (less than 100 days), skip calculation\n",
    "    try:\n",
    "        # prepare model, we are multiplying by 100 to express return in percentage\n",
    "        model = arch_model(series.dropna() * 100, vol='Garch', p=1, q=1, dist='normal')\n",
    "\n",
    "        # fit garch model to estimate volatility\n",
    "        res = model.fit(disp='off')\n",
    "\n",
    "        # gets model predicted volatility patterns\n",
    "        forecasts = res.conditional_volatility / 100  # Scale back to normal scale\n",
    "\n",
    "        forecasts = forecasts.shift(1)  # Use today's forecast for tomorrow's decision (avoid lookahead bias)\n",
    "        \n",
    "        forecasts.index = series.dropna().index # align forecast back to original dates\n",
    "        return forecasts.reindex(series.index) # reindex to fill missing dates\n",
    "    except:\n",
    "        return pd.Series(index=series.index, data=np.nan)\n",
    "\n",
    "# Compute GARCH Volatility Safely and Align Index\n",
    "garch_vol_series = (\n",
    "    df_merged.groupby('permno')['retx']\n",
    "    .apply(calculate_garch_vol)\n",
    "    .reset_index(level=0, drop=True)  # This flattens the index to match df_merged\n",
    ")\n",
    "\n",
    "# Now safely assign to the dataframe\n",
    "df_merged['garch_vol'] = garch_vol_series\n",
    "\n",
    "df_merged['garch_vol_annualized'] = df_merged['garch_vol'] * np.sqrt(252) # annualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467ad033-dac2-47ac-82fe-1233419c256b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample ticker for visualization\n",
    "sample_ticker = tickers_with_full_span[2]  # Pick first ticker with full span\n",
    "\n",
    "subset = df_merged[df_merged['ticker'] == sample_ticker]\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(subset['date'], subset['rolling_vol_20d'], label='Rolling Vol (20D)', alpha=0.7)\n",
    "plt.plot(subset['date'], subset['garch_vol_annualized'], label='GARCH Volatility', alpha=0.7)\n",
    "plt.title(f\"Volatility Comparison for {sample_ticker}\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Volatility\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053bce41-4a9e-4ff3-a488-ed1cc07ad06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## to compare which prediction is better\n",
    "\n",
    "# Calculate Future Realized Volatility (5-Day)\n",
    "df_merged['realized_vol_5d'] = ( # We are trying to predict the volatility for 5 days\n",
    "    df_merged.groupby('permno')['retx']\n",
    "    .rolling(window=5, min_periods=5)\n",
    "    .std()   # standard deviation (avg volatility of next 5 days)\n",
    "    .shift(-5)  # Look ahead 5 days\n",
    "    .reset_index(level=0, drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a1749b-a907-4df8-9109-9c83d54391ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = df_merged.dropna(subset=['garch_vol', 'rolling_vol_20d_daily', 'realized_vol_5d'])\n",
    "\n",
    "mse_garch = mean_squared_error(valid_data['realized_vol_5d'], valid_data['garch_vol'])\n",
    "mse_rolling = mean_squared_error(valid_data['realized_vol_5d'], valid_data['rolling_vol_20d_daily'])\n",
    "\n",
    "print(f\"üìâ GARCH Forecast MSE: {mse_garch:.6f}\")\n",
    "print(f\"üìâ Rolling Volatility MSE: {mse_rolling:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d52b14-86f4-4d2b-80de-f029c7f1521a",
   "metadata": {},
   "source": [
    "#### üìä **Forecast Accuracy Comparison for AAPL**\n",
    "\n",
    "| Model         | MSE (Lower is Better) |\n",
    "|----------------|-----------------------|\n",
    "| GARCH Volatility | 0.000882             |\n",
    "| Rolling Volatility | 0.000918           |\n",
    "\n",
    "- GARCH provided slightly better predictions of future volatility, but the difference is very small.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbd753f-5a4a-43b7-bf39-70c1f6e05026",
   "metadata": {},
   "source": [
    "### Now let's look over entire stocks\n",
    "\n",
    "**Analyze**:\n",
    "   - For how many stocks does GARCH volatility provide better predictions?\n",
    "   - For how many stocks does Rolling volatility perform better?\n",
    "   - Compute the **average MSE** for both methods to assess overall performance.\n",
    "\n",
    "This will help to decide which volatility model performs better at the stock level and across the entire market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b0d4e9-86b5-4171-859b-8018e33029bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE for Each Stock (permno)\n",
    "mse_results = []\n",
    "\n",
    "for permno, group in df_merged.groupby('permno'):\n",
    "    data = group.dropna(subset=['garch_vol', 'rolling_vol_20d_daily', 'realized_vol_5d'])\n",
    "    \n",
    "    if not data.empty:\n",
    "        mse_garch = mean_squared_error(data['realized_vol_5d'], data['garch_vol'])\n",
    "        mse_rolling = mean_squared_error(data['realized_vol_5d'], data['rolling_vol_20d_daily'])\n",
    "        \n",
    "        mse_results.append({'permno': permno, 'mse_garch': mse_garch, 'mse_rolling_20d': mse_rolling})\n",
    "\n",
    "# Convert to DataFrame\n",
    "mse_df = pd.DataFrame(mse_results)\n",
    "df_mse_summary = mse_df.copy()  # This dataframe stores one row per stock with MSE values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd888024-2ef6-4e4a-b13b-1f76c83209b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Results\n",
    "garch_better_count = (df_mse_summary['mse_garch'] < df_mse_summary['mse_rolling_20d']).sum()\n",
    "rolling_better_count = (df_mse_summary['mse_rolling_20d'] <= df_mse_summary['mse_garch']).sum()\n",
    "\n",
    "avg_mse_garch = df_mse_summary['mse_garch'].mean()\n",
    "avg_mse_rolling = df_mse_summary['mse_rolling_20d'].mean()\n",
    "\n",
    "print(f\"üìà Number of stocks where GARCH performed better: {garch_better_count}\")\n",
    "print(f\"üìâ Number of stocks where Rolling 20D performed better: {rolling_better_count}\")\n",
    "print(f\"\\n‚úÖ Average MSE - GARCH: {avg_mse_garch:.6f}\")\n",
    "print(f\"‚úÖ Average MSE - Rolling 20D: {avg_mse_rolling:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143bfc05-0aa3-4f5d-ae7b-34d543c92ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_stocks = df_merged['permno'].nunique()\n",
    "print(f\"üìä Total Unique Stocks in the Dataset: {unique_stocks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f0486d-b696-4b97-9657-d6a56beded5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mse_summary.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c2fc8d-85de-47fb-bc81-946328957a2d",
   "metadata": {},
   "source": [
    "### üìñ **Volatility Forecast Evaluation ‚Äì Final Results**\n",
    "\n",
    "Out of a total of **11,827 unique stocks** in the dataset, only **10,828 stocks had sufficient and valid data** to perform a meaningful comparison between GARCH and Rolling 20-Day volatility forecasts.  \n",
    "The remaining stocks lacked enough historical price data or had missing values required for computing as:\n",
    "- GARCH Volatility (requires at least 100 valid return observations),\n",
    "- Rolling 20-Day Volatility (requires at least 15 observations),\n",
    "- Future Realized Volatility (needs returns data for the next 5 days).\n",
    "\n",
    "\n",
    "\n",
    "#### üìä **Results Summary:**\n",
    "\n",
    "| Metric                              | Value        |\n",
    "|--------------------------------------|--------------|\n",
    "| Stocks Where GARCH Performed Better  | **10,828**   |\n",
    "| Stocks Where Rolling Performed Better | **0**        |\n",
    "| Average MSE ‚Äì GARCH                  | **0.001301** |\n",
    "| Average MSE ‚Äì Rolling 20D            | **0.639405** |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1436758a-a064-4871-9190-cb5f9c663d3c",
   "metadata": {},
   "source": [
    "#### Let's compare GARCH with rolling vol for 5, 10 and 20 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4255a7a1-98b4-4344-8a48-a8fdd931699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_analyze_mse(df, horizons=[5, 10, 20], realized_col='realized_vol_5d'):\n",
    "    # Calculate Rolling Volatility for All Horizons (Daily)\n",
    "    \n",
    "    for h in horizons:\n",
    "        rolling_col = f'rolling_vol_{h}d_daily'\n",
    "        if rolling_col not in df.columns:        \n",
    "            df[rolling_col] = (\n",
    "                df.groupby('permno')['retx']\n",
    "                .rolling(window=h, min_periods=int(h * 0.75)) # looks at past 20 days returns to calculate today's volatility\n",
    "                .std()\n",
    "                .reset_index(level=0, drop=True)\n",
    "            )\n",
    "\n",
    "            \n",
    "\n",
    "    # Calculate and Analyze MSE for Each Horizon\n",
    "    for h in horizons:\n",
    "        rolling_col = f'rolling_vol_{h}d_daily'\n",
    "        mse_results = []\n",
    "\n",
    "        for permno, group in df_merged.groupby('permno'):\n",
    "            data = group.dropna(subset=['garch_vol', rolling_col, realized_col])\n",
    "            \n",
    "            if not data.empty:\n",
    "                mse_garch = mean_squared_error(data[realized_col], data['garch_vol'])\n",
    "                mse_rolling = mean_squared_error(data[realized_col], data[rolling_col])\n",
    "                \n",
    "                mse_results.append({'permno': permno, 'mse_garch': mse_garch, f'mse_rolling_{h}d': mse_rolling})\n",
    "\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        mse_df = pd.DataFrame(mse_results)\n",
    "        \n",
    "        if not mse_df.empty:\n",
    "            garch_better_count = (mse_df['mse_garch'] < mse_df[f'mse_rolling_{h}d']).sum()\n",
    "            rolling_better_count = (mse_df[f'mse_rolling_{h}d'] <= mse_df['mse_garch']).sum()\n",
    "\n",
    "            avg_mse_garch = mse_df['mse_garch'].mean()\n",
    "            avg_mse_rolling = mse_df[f'mse_rolling_{h}d'].mean()\n",
    "\n",
    "            print(f\"--- Horizon: {h} Days ---\")\n",
    "            print(f\"üìà Number of stocks where GARCH performed better: {garch_better_count}\")\n",
    "            print(f\"üìâ Number of stocks where Rolling {h}D performed better: {rolling_better_count}\")\n",
    "            print(f\"‚úÖ Average MSE - GARCH: {avg_mse_garch:.6f}\")\n",
    "            print(f\"‚úÖ Average MSE - Rolling {h}D: {avg_mse_rolling:.6f}\\n\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è No valid data found for horizon {h} days.\\n\")\n",
    "\n",
    "# Example Usage:\n",
    "calculate_and_analyze_mse(df_merged, horizons=[5, 10, 20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c416e7-22fc-4e5e-abee-aa22732243a7",
   "metadata": {},
   "source": [
    "> üìñ *Note: The difference in GARCH mse across different horizon is due to the changes in number of datapoints. This is because of the following column line: \n",
    "```python\n",
    "data = group.dropna(subset=['garch_vol', rolling_col, realized_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d219c6-efa4-4e1d-b4be-6bc598a6b18b",
   "metadata": {},
   "source": [
    "### üìå **Conclusion & Model Selection:**\n",
    "\n",
    "- GARCH Volatility provided consistently lower forecast errors across most stocks\n",
    "(especially 5-day horizon) with valid data.\n",
    "- Rolling 5, 10, 20-Day volatility not only performed slightly worse but with higher average MSE, indicating poor predictive accuracy.\n",
    "- ‚úÖ **Decision:**  \n",
    "We will use **GARCH Volatility** for further modeling and trading strategy development, as it has demonstrated superior predictive power in forecasting future realized volatility.\n",
    "\n",
    "> üìñ *Note: This choice aligns with our objective of improving risk-adjusted returns through better volatility estimation and position sizing.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1220aed6-a03f-4945-bebe-04f919b2e3bd",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cb4112-7f6e-4148-a615-fd334f28be4d",
   "metadata": {},
   "source": [
    "## üìñ **Ornstein-Uhlenbeck (OU) Return Forecast Analysis**\n",
    "\n",
    "Now that we have determined GARCH volatility provides better risk estimation, we evaluate the predictive power of the **Ornstein-Uhlenbeck (OU) process** for return forecasting.\n",
    "\n",
    "\n",
    "\n",
    "#### üìå **What We Have Implemented**\n",
    "\n",
    "We use the **Discrete-Time OU Forecast Formula** to estimate the expected return for the next day:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X_{t+1}] = X_t \\cdot e^{-\\theta} + \\mu \\cdot \\left(1 - e^{-\\theta}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "- $X_t$: This is the **previous day's return**, taken from the `retx` column.\n",
    "- We calculate $\\theta$ and $\\mu$ using the `estimate_ou_params` function.\n",
    "\n",
    "| Parameter    | Meaning                 | How We Calculated and Used It |\n",
    "|---------------|-------------------------|------------------------------|\n",
    "| $\\theta$    | **Speed of mean reversion**. Controls **how quickly returns move back toward their long-term average**. | Calculated **individually for each stock** using historical returns. Estimated via linear regression: first compute $\\beta$ (coefficient) from $X_t = \\beta X_{t-1} + c$, then calculate $\\theta = -\\ln(\\beta)$. |\n",
    "| $\\mu$       | **Long-term mean return**. This is the return level each stock tends to revert to over time. | Calculated **individually for each stock** using $\\mu = \\frac{c}{1 - \\beta}$, where $c$ is the intercept from the previous regression. |\n",
    "\n",
    "- These parameters are computed **separately for each stock** based on its historical return data.\n",
    "\n",
    "\n",
    "After calculating these parameters, we forecast the next day's return and store it in the `ou_forecast_return` column using:\n",
    "\n",
    "```python\n",
    "df_merged['ou_forecast_return'] = (\n",
    "    df_merged['retx'].shift(1) * np.exp(-df_merged['theta']) +\n",
    "    df_merged['mu'] * (1 - np.exp(-df_merged['theta']))\n",
    ")\n",
    "```\n",
    "\n",
    "#### **Explanation of `np.exp(-Œ∏)` in the Formula**\n",
    "\n",
    "- `np.exp(-Œ∏)` calculates the mathematical term $e^{-\\theta}$.\n",
    "- This term determines **how much weight is given to the previous day's return ($X_t$)** in the forecast.\n",
    "    - If $\\theta$ is small, $e^{-\\theta}$ is close to 1, and the forecast relies more on the previous return.\n",
    "    - If $\\theta$ is large, $e^{-\\theta}$ becomes small, and the forecast relies more on the long-term mean $\\mu$.\n",
    "\n",
    "- In simple terms:\n",
    "    - A **small $\\theta$** means slow mean reversion, so the forecast stays closer to yesterday‚Äôs return.\n",
    "    - A **large $\\theta$** means fast mean reversion, so the forecast moves quickly toward the historical average return.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acda8dc-8bce-45ac-ab75-b4b9bf82d963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OU Parameter Estimation Function\n",
    "def estimate_ou_params(series):\n",
    "    series = series.dropna()\n",
    "    if len(series) < 30:\n",
    "        return None, None  # Insufficient data\n",
    "\n",
    "    x_lag = series.shift(1).dropna()\n",
    "    x_current = series.loc[x_lag.index]\n",
    "\n",
    "    X = np.vstack([x_lag.values, np.ones(len(x_lag))]).T\n",
    "    beta, c = np.linalg.lstsq(X, x_current.values, rcond=None)[0]\n",
    "\n",
    "    if beta <= 0 or beta >= 1:\n",
    "        return None, None  # Invalid beta, no meaningful mean reversion\n",
    "\n",
    "    Œ∏ = -np.log(beta)  # Speed of mean reversion\n",
    "    Œº = c / (1 - beta)  # Long-term mean level\n",
    "\n",
    "    return Œ∏, Œº\n",
    "\n",
    "# Apply OU Estimation for Each Stock\n",
    "ou_results = []\n",
    "\n",
    "for permno, group in df_merged.groupby('permno'):\n",
    "    ret_series = group['retx']\n",
    "    Œ∏, Œº = estimate_ou_params(ret_series)\n",
    "    if Œ∏ is not None and Œº is not None:\n",
    "        ou_results.append({'permno': permno, 'theta': Œ∏, 'mu': Œº})\n",
    "\n",
    "ou_df = pd.DataFrame(ou_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b634d0f9-12c2-4500-a610-edcaf198cd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge OU Parameters Back to Main Data\n",
    "df_merged = df_merged.merge(ou_df, on='permno', how='left')\n",
    "\n",
    "# OU Return Forecast Formula: E[x_t+1] = x_t * exp(-Œ∏) + Œº * (1 - exp(-Œ∏))\n",
    "df_merged['ou_forecast_return'] = (\n",
    "    df_merged['retx'].shift(1) * np.exp(-df_merged['theta']) +\n",
    "    df_merged['mu'] * (1 - np.exp(-df_merged['theta']))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0413dfe-699a-401d-b89e-8dbdb4cc4736",
   "metadata": {},
   "source": [
    "#### OU based z-score vs classical z-score (based on SD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b23f3a-d748-4eb2-9d4c-92c923afb17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìå Calculate OU-Based Z-Score\n",
    "df_merged['z_ou'] = (\n",
    "    (df_merged['ou_forecast_return'] - df_merged['retx']) / df_merged['garch_vol']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781f80eb-a9c2-4632-8b53-c4e0eedeafd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_zscore_with_ou(df, horizons=[5, 10, 20]):\n",
    "    results = []\n",
    "    \n",
    "    for h in horizons:\n",
    "        # üìå Calculate Rolling Mean and Std Dev for Classical Z-Score\n",
    "        rolling_mean = (\n",
    "            df.groupby('permno')['retx']\n",
    "            .rolling(window=h, min_periods=int(h * 0.75))\n",
    "            .mean()\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "        \n",
    "        rolling_std = (\n",
    "            df.groupby('permno')['retx']\n",
    "            .rolling(window=h, min_periods=int(h * 0.75))\n",
    "            .std()\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "        \n",
    "        # Classical Z-Score for Current Horizon\n",
    "        z_col = f'z_classical_{h}d'\n",
    "        df[z_col] = (df['retx'] - rolling_mean) / rolling_std\n",
    "        df[f'{z_col}_shifted'] = df[z_col].shift(1)  # Apply shift to avoid lookahead bias\n",
    "\n",
    "        # Ensure OU-Based Z-Score is Shifted\n",
    "        if 'z_ou_shifted' not in df.columns:\n",
    "            df['z_ou_shifted'] = df['z_ou'].shift(1)\n",
    "\n",
    "        # Prepare Valid Dataset with Both Classical and OU-Based Z-Scores Available\n",
    "        valid = df.dropna(subset=[f'{z_col}_shifted', 'z_ou_shifted', 'future_return_1d'])\n",
    "\n",
    "        # Calculate Hit Rates\n",
    "        hit_rate_classical = (np.sign(valid[f'{z_col}_shifted']) == np.sign(valid['future_return_1d'])).mean()\n",
    "        hit_rate_ou = (np.sign(valid['z_ou_shifted']) == np.sign(valid['future_return_1d'])).mean()\n",
    "\n",
    "        results.append({\n",
    "            'Hit Rate Classical (%)': hit_rate_classical * 100,\n",
    "            'Hit Rate OU-Based (%)': hit_rate_ou * 100,\n",
    "            'Unique Permnos': valid['permno'].nunique()\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Ensure Future Return is Calculated\n",
    "df_merged['future_return_1d'] = df_merged.groupby('permno')['retx'].shift(-1)\n",
    "\n",
    "# Run Evaluation\n",
    "evaluation_df = evaluate_zscore_with_ou(df_merged, horizons=[5, 10, 20])\n",
    "\n",
    "evaluation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b32598-60b8-4db3-aa5f-a99887467a0d",
   "metadata": {},
   "source": [
    "#### üìå **Incorporating Volatility into OU Forecast**\n",
    "\n",
    "In the deterministic OU forecast, we calculated only the expected mean return using:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X_{t+1}] = X_t \\cdot e^{-\\theta} + \\mu \\cdot \\left(1 - e^{-\\theta}\\right)\n",
    "$$\n",
    "\n",
    "However, this ignores the randomness or uncertainty in returns.  \n",
    "To introduce volatility into the forecast, we simulate a random shock using the following stochastic version of the OU process:\n",
    "\n",
    "$$\n",
    "X_{t+1} = \\mathbb{E}[X_{t+1}] + \\sigma \\cdot \\sqrt{1 - e^{-2\\theta}} \\cdot \\varepsilon\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbb{E}[X_{t+1}]$: Deterministic OU forecast (`ou_forecast_return`).\n",
    "- $\\sigma$: Volatility estimate, using **GARCH volatility** (`garch_vol`).\n",
    "- $\\theta$: Mean reversion speed parameter.\n",
    "- $\\varepsilon \\sim \\mathcal{N}(0, 1)$: A random value drawn from a standard normal distribution.\n",
    "\n",
    "\n",
    "\n",
    "- `np.random.normal(0, 1, len(df_merged))` generates random shocks for each time period.\n",
    "- `np.sqrt(1 - np.exp(-2 * df_merged['theta']))` adjusts the variance to account for discrete time steps.\n",
    "- This creates a more realistic forecast by adding randomness, representing possible price movement uncertainty around the mean forecast.\n",
    "\n",
    "The final stochastic forecast is stored in `ou_forecast_with_volatility`:\n",
    "\n",
    "```python\n",
    "df_merged['ou_forecast_with_volatility'] = (\n",
    "    df_merged['ou_forecast_return'] + \n",
    "    df_merged['garch_vol'] * np.sqrt(1 - np.exp(-2 * df_merged['theta'])) * np.random.normal(0, 1, len(df_merged))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077ebb8d-c9a0-4493-9dae-db6b9e6e88ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you already have ou_forecast_return, theta, and GARCH volatility columns\n",
    "\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "df_merged['ou_forecast_with_volatility'] = (\n",
    "    df_merged['ou_forecast_return'] + \n",
    "    df_merged['garch_vol'] * np.sqrt(1 - np.exp(-2 * df_merged['theta'])) * np.random.normal(0, 1, len(df_merged))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9ab2b9-e266-4c6d-9c8f-e3ed50512c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['z_ou_with_volatility'] = (\n",
    "    (df_merged['ou_forecast_with_volatility'] - df_merged['retx']) / df_merged['garch_vol']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5708b11-b6bb-41da-906f-0a4936b16e03",
   "metadata": {},
   "source": [
    "### üìñ **Evaluating Hit Rate for Trading Signals**\n",
    "\n",
    "In this analysis, we evaluate the effectiveness of our trading signals by calculating the **Hit Rate** across different forecasting methods and time horizons.\n",
    "\n",
    "\n",
    "\n",
    "#### üìå **What Is Hit Rate?**\n",
    "\n",
    "- Hit Rate measures the **percentage of times the trading signal correctly predicts the direction of future returns**.\n",
    "- It focuses **only on the direction (up or down)**, not the magnitude of returns.\n",
    "- A higher Hit Rate indicates that the signal is better at predicting whether the return will be positive or negative.\n",
    "\n",
    "$$\n",
    "\\text{Hit Rate} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Predictions}}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "We will use **Z-Scores** to generate signals for hit rate prediction.  \n",
    "- A **positive Z-Score** indicates that the stock is expected to **rise** (suggesting a long position).  \n",
    "- A **negative Z-Score** indicates that the stock is expected to **fall** (suggesting a short position).  \n",
    "\n",
    "By comparing the predicted direction from Z-Scores with the actual future return, we can measure how many predictions were correct and how many were wrong.\n",
    "\n",
    "\n",
    "\n",
    "#### üìö **Z-Scores We Are Comparing:**\n",
    "\n",
    "\n",
    "\n",
    "#### 1Ô∏è‚É£ **Classical Z-Score**\n",
    "\n",
    "- Formula:\n",
    "\n",
    "$$\n",
    "Z_{\\text{Classical}} = \\frac{X_t - \\mu_{\\text{rolling}}}{\\sigma_{\\text{rolling}}}\n",
    "$$\n",
    "\n",
    "- Where:\n",
    "  - $X_t$ = Current return (`retx`).\n",
    "  - $\\mu_{\\text{rolling}}$ = Rolling mean of returns over a defined horizon (e.g., 5, 10, 20 days).\n",
    "  - $\\sigma_{\\text{rolling}}$ = Rolling standard deviation of returns over the same horizon.\n",
    "\n",
    "\n",
    "\n",
    "#### 2Ô∏è‚É£ **OU-Based Z-Score (Deterministic)**\n",
    "\n",
    "- Formula:\n",
    "\n",
    "$$\n",
    "Z_{\\text{OU}} = \\frac{\\mathbb{E}[X_{t+1}] - X_t}{\\text{Volatility Estimate}}\n",
    "$$\n",
    "\n",
    "- Where:\n",
    "  - $\\mathbb{E}[X_{t+1}]$ = Forecasted return using the OU model (deterministic forecast).\n",
    "    - Computed as:\n",
    "\n",
    "    $$\n",
    "    \\mathbb{E}[X_{t+1}] = X_t \\cdot e^{-\\theta} + \\mu \\cdot \\left(1 - e^{-\\theta}\\right)\n",
    "    $$\n",
    "\n",
    "  - $X_t$ = Current return (`retx`).\n",
    "  - $\\theta$ = Speed of mean reversion.\n",
    "  - $\\mu$ = Long-term average return (mean-reversion level).\n",
    "  - Volatility Estimate = Typically GARCH-based volatility estimate.\n",
    "\n",
    "\n",
    "#### 3Ô∏è‚É£ **OU-Based Z-Score (With Volatility)**\n",
    "\n",
    "- Formula:\n",
    "\n",
    "$$\n",
    "Z_{\\text{OU, Stochastic}} = \\frac{X_{t+1}^{\\text{stochastic}} - X_t}{\\text{Volatility Estimate}}\n",
    "$$\n",
    "\n",
    "- Where:\n",
    "  - $X_{t+1}^{\\text{stochastic}} = \\mathbb{E}[X_{t+1}] + \\sigma \\cdot \\sqrt{1 - e^{-2\\theta}} \\cdot \\varepsilon$\n",
    "  - $\\varepsilon \\sim \\mathcal{N}(0, 1)$ is a random shock (introduces randomness to the forecast).\n",
    "  - $\\sigma$ = GARCH-based volatility estimate.\n",
    "\n",
    "\n",
    "\n",
    "By calculating and comparing these Z-Scores, we can assess how well each method predicts the direction of returns, using Hit Rate as the evaluation metric.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7c84c5-c567-4e0a-9b65-0c7942d2ed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_zscore_with_ou(df, horizons=[5, 10, 20]):\n",
    "    results = []\n",
    "    \n",
    "    for h in horizons:\n",
    "        # üìå Calculate Rolling Mean and Std Dev for Classical Z-Score\n",
    "        rolling_mean = (\n",
    "            df.groupby('permno')['retx']\n",
    "            .rolling(window=h, min_periods=int(h * 0.75))\n",
    "            .mean()\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "        \n",
    "        rolling_std = (\n",
    "            df.groupby('permno')['retx']\n",
    "            .rolling(window=h, min_periods=int(h * 0.75))\n",
    "            .std()\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "        \n",
    "        # üìå Classical Z-Score for Current Horizon\n",
    "        z_col = f'z_classical_{h}d'\n",
    "        df[z_col] = (df['retx'] - rolling_mean) / rolling_std\n",
    "        df[f'{z_col}_shifted'] = df[z_col].shift(1)  # Avoid lookahead bias\n",
    "\n",
    "        # üìå Ensure OU-Based Z-Scores Are Shifted\n",
    "        df['z_ou_shifted'] = df['z_ou'].shift(1)\n",
    "        df['z_ou_with_volatility_shifted'] = df['z_ou_with_volatility'].shift(1)\n",
    "\n",
    "        # üìå Prepare Valid Dataset with All Required Columns\n",
    "        valid = df.dropna(subset=[\n",
    "            f'{z_col}_shifted', \n",
    "            'z_ou_shifted', \n",
    "            'z_ou_with_volatility_shifted', \n",
    "            'future_return_1d'\n",
    "        ])\n",
    "\n",
    "        # üìà Calculate Hit Rates (Direction Prediction Accuracy)\n",
    "        hit_rate_classical = (np.sign(valid[f'{z_col}_shifted']) == np.sign(valid['future_return_1d'])).mean()\n",
    "        hit_rate_ou_det = (np.sign(valid['z_ou_shifted']) == np.sign(valid['future_return_1d'])).mean()\n",
    "        hit_rate_ou_stochastic = (np.sign(valid['z_ou_with_volatility_shifted']) == np.sign(valid['future_return_1d'])).mean()\n",
    "\n",
    "        result_row = {\n",
    "            'Horizon (Days)': h,\n",
    "            'Hit Rate Classical (%)': hit_rate_classical * 100,\n",
    "            'Hit Rate OU (Deterministic) (%)': hit_rate_ou_det * 100,\n",
    "            'Hit Rate OU (With Volatility) (%)': hit_rate_ou_stochastic * 100,\n",
    "            'Unique Permnos': valid['permno'].nunique()\n",
    "        }\n",
    "\n",
    "        results.append(result_row)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Ensure Future Return is Calculated\n",
    "df_merged['future_return_1d'] = df_merged.groupby('permno')['retx'].shift(-1)\n",
    "\n",
    "# Run Evaluation\n",
    "evaluation_df = evaluate_zscore_with_ou(df_merged, horizons=[5, 10, 20])\n",
    "\n",
    "evaluation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9d7f73-0b2d-4d4b-b01c-a4cf06560e81",
   "metadata": {},
   "source": [
    "#### **Final Decision: Use Classical Z-Score with 5-Day Horizon**\n",
    "\n",
    "Based on our evaluation, we observed that the **Classical Z-Score with a 5-day horizon** performs almost as well as the OU-based Z-Scores in terms of directional prediction (Hit Rate), while being much simpler to compute and implement.\n",
    "\n",
    "However, the hit rate is only 46.58% of time, meaning the z-score direction prediction is only correct for 46.58% of the time. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fed92-4918-4af9-bfe6-6be6a97c1503",
   "metadata": {},
   "source": [
    "### **Analyzing hit rate for Modeling**\n",
    "Now, that we have decided to take classical z-score let's analyze it to form a better strategy\n",
    "\n",
    "#### Let's begin with hit rate based on permno (individual security)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80adeac-e0eb-487e-8de0-e0813e82ac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure Z-Score and future return columns are properly shifted and available\n",
    "df_merged['z_classical_5d_shifted'] = df_merged['z_classical_5d'].shift(1)\n",
    "df_merged['future_return_1d'] = df_merged.groupby('permno')['retx'].shift(-1)\n",
    "\n",
    "# Filter out rows with missing values\n",
    "valid_data = df_merged.dropna(subset=['z_classical_5d_shifted', 'future_return_1d'])\n",
    "\n",
    "# Calculate Hit Indicator: 1 if correct direction, 0 otherwise\n",
    "valid_data['hit_correct'] = (np.sign(valid_data['z_classical_5d_shifted']) == \n",
    "                             np.sign(valid_data['future_return_1d'])).astype(int)\n",
    "\n",
    "# Calculate Hit Rate per permno\n",
    "hit_rate_per_stock = valid_data.groupby('permno')['hit_correct'].mean().reset_index()\n",
    "hit_rate_per_stock.rename(columns={'hit_correct': 'Hit Rate'}, inplace=True)\n",
    "\n",
    "# Sort Stocks by Hit Rate (Descending)\n",
    "hit_rate_per_stock = hit_rate_per_stock.sort_values('Hit Rate', ascending=False)\n",
    "\n",
    "hit_rate_per_stock.head(10)  # Top 10 stocks with highest Hit Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5418fb91-b99e-4d3c-a815-b2da4b9bf12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of permnos with Hit Rate greater than 70%\n",
    "num_high_hit_rate_stocks = (hit_rate_per_stock['Hit Rate'] > 0.50).sum()\n",
    "\n",
    "print(f\"Number of stocks with Hit Rate > 50%: {num_high_hit_rate_stocks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff0c5c5-2ada-42ef-ba06-09658143c422",
   "metadata": {},
   "source": [
    "#### üìñ **Does Speed and Long-Term Mean Have Any Relationship with Hit Rate?**\n",
    "\n",
    "The **speed** ($\\theta$) and **long-term mean** ($\\mu$) are parameters in the **Ornstein-Uhlenbeck (OU) forecast**, which models mean-reverting behavior of stock returns.\n",
    "\n",
    "\n",
    "\n",
    "- **$X_t$ (Current Value):**  \n",
    "  - In our implementation, $X_t$ represents the **daily price return excluding dividends**, calculated from stock price changes.  \n",
    "  - This is the value from the `retx` column in the dataset.\n",
    "\n",
    "\n",
    "\n",
    "- **Speed ($\\theta$):**  \n",
    "  - Represents **how quickly returns revert to their long-term mean**.  \n",
    "  - Higher $\\theta$ ‚Üí Faster reversion.  \n",
    "  - Calculated from the regression equation:  \n",
    "\n",
    "    $$\n",
    "    \\theta = -\\ln(\\beta)\n",
    "    $$  \n",
    "\n",
    "    where $\\beta$ is the slope coefficient from the regression equation:\n",
    "\n",
    "    $$\n",
    "    X_t = \\beta X_{t-1} + c\n",
    "    $$  \n",
    "\n",
    "- **Long-Term Mean ($\\mu$):**  \n",
    "  - Represents the **average return level** the stock tends to revert to over time.  \n",
    "  - Calculated as:  \n",
    "\n",
    "    $$\n",
    "    \\mu = \\frac{c}{1 - \\beta}\n",
    "    $$  \n",
    "\n",
    "    where $c$ is the intercept from the same regression.\n",
    "\n",
    "\n",
    "\n",
    "Let's try to visualize their relationships with Hit Rate to check if we can incorporate them into our trading strategy for improving signal effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bfeebe-282f-4eaf-9968-033f0752fbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_analysis_df = hit_rate_per_stock.merge(ou_df, on='permno', how='inner')\n",
    "hit_analysis_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51fb34b-8c08-42ba-b9cb-25bf912779e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.scatterplot(data=hit_analysis_df, x='theta', y='Hit Rate')\n",
    "plt.title(\"Relationship Between Mean Reversion Speed (Œ∏) and Signal Hit Rate\")\n",
    "plt.xlabel(\"Theta (Speed of Mean Reversion)\")\n",
    "plt.ylabel(\"Z-Score Hit Rate\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec52986a-5125-4084-a9cc-8c6920e60500",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = hit_analysis_df[['theta', 'Hit Rate']].corr().iloc[0,1]\n",
    "print(f\"Correlation between theta and Hit Rate: {correlation:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690a5591-43d7-4b25-af1c-0e132095e91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.scatterplot(data=hit_analysis_df, x='mu', y='Hit Rate')\n",
    "plt.title(\"Relationship Between Long-Term Mean ($\\mu$) and Hit Rate\")\n",
    "plt.xlabel(\"Mu (Long-Term Mean Return)\")\n",
    "plt.ylabel(\"Z-Score Hit Rate\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f43669-4ad1-4192-8e4e-bc2ef9761c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_mu = hit_analysis_df[['mu', 'Hit Rate']].corr().iloc[0, 1]\n",
    "print(f\"Correlation between mu and Hit Rate: {correlation_mu:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8a0081-41da-4082-ace0-538558c84931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binomtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a1f913-6bfe-4069-8823-d385f69be4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'valid_data' has 'permno' and 'hit_correct' columns\n",
    "\n",
    "# Calculate total trades and correct predictions per stock\n",
    "hit_summary = valid_data.groupby('permno').agg(\n",
    "    correct_predictions=('hit_correct', 'sum'),\n",
    "    total_trades=('hit_correct', 'count')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate p-values for each stock\n",
    "hit_summary['p_value'] = hit_summary.apply(\n",
    "    lambda row: binomtest(int(row['correct_predictions']), int(row['total_trades']), p=0.5, alternative='greater').pvalue,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Filter for statistically significant stocks\n",
    "significant_stocks = hit_summary[hit_summary['p_value'] < 0.05]\n",
    "\n",
    "significant_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e19267-4c83-4569-8b5e-fe53bc9bc3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "significance_rate = (hit_summary['p_value'] < 0.05).mean() * 100\n",
    "print(f\"Percentage of Stocks with Significant Hit Rates: {significance_rate:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fa206c-f874-46c9-95a6-e3f89eb78d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3852d809-c094-4f6f-972e-17e5d2c9aa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3684b0-dfcd-4d97-abb8-977428bed665",
   "metadata": {},
   "source": [
    "### üìñ Analysis: Investigating the Impact of GARCH Volatility on Trading Signal Accuracy \n",
    "\n",
    "In this analysis, we explore whether the level of GARCH-estimated volatility has any relationshp with the accuracy of trading signals generated using classical SD based z-scores.\n",
    "\n",
    "- Calculate the correctness of each prediction by comparing the predicted return direction (from lagged z-scores) with the actual future return.\n",
    "- Use GARCH volatility to segment the data into 10 equal-sized buckets (deciles), from lowest to highest volatility periods.\n",
    "- Calculate and plot the hit rate for each volatility decile to observe how signal accuracy changes with market volatility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f49b0b-d075-46fe-a4c9-f21d93810a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing data\n",
    "valid_data = df_merged.dropna(subset=['z_classical_5d_shifted', 'future_return_1d']).copy()\n",
    "\n",
    "# Calculate the hit correctness\n",
    "valid_data.loc[:, 'hit_correct'] = (\n",
    "    np.sign(valid_data['z_classical_5d_shifted']) == np.sign(valid_data['future_return_1d'])\n",
    ").astype(int)\n",
    "\n",
    "# Bucket GARCH volatility within valid_data only (not df_merged)\n",
    "valid_data.loc[:, 'garch_vol_bucket'] = pd.qcut(valid_data['garch_vol'], q=10, labels=False)\n",
    "\n",
    "# Calculate hit rate by GARCH volatility bucket\n",
    "hit_rate_by_vol = valid_data.groupby('garch_vol_bucket')['hit_correct'].mean().reset_index()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.lineplot(data=hit_rate_by_vol, x='garch_vol_bucket', y='hit_correct')\n",
    "plt.xlabel(\"GARCH Volatility Decile (0 = Low Vol, 9 = High Vol)\")\n",
    "plt.ylabel(\"Hit Rate\")\n",
    "plt.title(\"Hit Rate vs. GARCH Volatility Buckets\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedf5cf0-0c1b-4efb-bd36-2bf1739cafbe",
   "metadata": {},
   "source": [
    "Even though there is a clear signal that decrease in the GARCH volatility Decile shows strong z-score direction, hit rate is still below 50% for the first decile. So, let's create 100 quantile-based buckets to get clear overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84b8ad2-8980-4199-a0e1-39c00e55f656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 100 quantile-based buckets\n",
    "valid_data.loc[:, 'garch_vol_bucket_100'] = pd.qcut(valid_data['garch_vol'], q=100, labels=False)\n",
    "\n",
    "# Calculate hit rate by the new 100 buckets\n",
    "hit_rate_by_vol_100 = valid_data.groupby('garch_vol_bucket_100')['hit_correct'].mean().reset_index()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.lineplot(data=hit_rate_by_vol_100, x='garch_vol_bucket_100', y='hit_correct')\n",
    "plt.xlabel(\"GARCH Volatility Percentile (0 = Lowest Vol, 99 = Highest Vol)\")\n",
    "plt.ylabel(\"Hit Rate\")\n",
    "plt.title(\"Hit Rate vs. GARCH Volatility Percentiles (100 Buckets)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d090750-7104-43c5-830e-40f0a8ad18e3",
   "metadata": {},
   "source": [
    "### üìä Summary \n",
    "\n",
    "Even though theres is no significant difference between 100 quantile hit rates and 10 deciles hit rate, the charts clearly shows that as GARCH volatility increases, the hit rate of our trading signals declines. This indicates that our z-score signals are more effective in **low volatility environments** and tend to perform poorly during **high volatility periods**.\n",
    "\n",
    "#### ‚úÖ **Implications for Modeling:**\n",
    "- **Trade Filtering:**  \n",
    "  Execute trades only when GARCH volatility is within lower deciles (e.g., 0‚Äì3), avoiding high volatility periods where signals are less reliable.\n",
    "  \n",
    "- **Position Sizing:**  \n",
    "  Apply **inverse volatility weighting** to allocate larger positions during low volatility regimes and smaller positions during high volatility regimes.\n",
    "\n",
    "- **Dynamic Thresholding:**  \n",
    "  Adjust z-score entry thresholds based on volatility regimes‚Äîrequire stronger signals during high volatility periods to reduce false positives.\n",
    "\n",
    "Incorporating GARCH volatility into the trading framework allows for better risk management and enhances the effectiveness of mean-reversion strategies by focusing on periods where the strategy has historically performed better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61530333-d5fa-416b-81ab-3b7e0fc43acf",
   "metadata": {},
   "source": [
    "### **Mean Reversion Analysis**\n",
    "### For further analysis let's only take relavant columns and non nan rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db112a1a-ca55-46fc-be1a-da63032c31c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required columns\n",
    "required_columns = ['rolling_vol_5d_daily', 'garch_vol', 'garch_vol_annualized', \n",
    "                    'z_classical_5d', 'z_classical_5d_shifted']\n",
    "\n",
    "# Filter columns that have 0 nulls and are in required_columns\n",
    "columns_with_no_null = df_merged.columns[(df_merged.isnull().sum() == 0)]\n",
    "print(columns_with_no_null)\n",
    "final_columns = required_columns + list(columns_with_no_null)\n",
    "\n",
    "# Final filtered DataFrame\n",
    "df_filtered = df_merged[final_columns]\n",
    "\n",
    "df_filtered = df_filtered.dropna()\n",
    "\n",
    "df_filtered.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070dfc1a-d043-4e66-889e-f21d74509b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecc5752-d163-4172-a47d-bfb317a90dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average return for each group_id and date\n",
    "df_filtered['group_avg_return'] = df_filtered.groupby(['group_id', 'date'])['retx'].transform('mean')\n",
    "\n",
    "# Calculate each stock's return relative to its group average\n",
    "df_filtered['retx_relative'] = df_filtered['retx'] - df_filtered['group_avg_return']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f96c1fd-4f53-4bc7-9a23-b7f4b5a8c06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank stocks within group\n",
    "df_filtered['group_rank'] = df_filtered.groupby(['group_id', 'date'])['retx'].rank()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21572c47-0f9d-48f4-9286-b1a323443400",
   "metadata": {},
   "source": [
    "### üìñ After Extreme Deviations\n",
    "\n",
    "In this analysis, we investigate whether stocks that significantly deviate from their peer group‚Äôs average return (`retx_relative`) tend to mean revert in the short term.\n",
    "\n",
    "\n",
    "#### üß™ Approach\n",
    "\n",
    "- Identify extreme values of `retx_relative` using the top and bottom 5% thresholds.\n",
    "- Calculate future cumulative returns over multiple holding periods (1, 3, 5, and 10 days).\n",
    "- Plot average future returns following:\n",
    "  - **Long positions** (stocks that underperformed their group ‚Äî bottom 5%)\n",
    "  - **Short positions** (stocks that outperformed their group ‚Äî top 5%)\n",
    "\n",
    "This allows us to evaluate whether such relative return extremes are followed by price corrections, i.e., mean reversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdc2ad9-84b0-4600-9ee0-7621e9412ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define extreme thresholds (e.g., top and bottom 5% relative returns)\n",
    "lower_threshold = df_filtered['retx_relative'].quantile(0.10)\n",
    "upper_threshold = df_filtered['retx_relative'].quantile(0.90)\n",
    "\n",
    "print(lower_threshold)\n",
    "print(upper_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a88880-0c37-408e-9706-30850238d11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_filtered['retx_relative'] > 0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2914b5d-2c58-424f-8bfa-9b4ab9fb2b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered['retx'].quantile(0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77e5dbe-b68f-4838-b26b-af9d8d8dd169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create future return columns\n",
    "for h in [1, 3, 5, 10]:  # 1-day, 3-day, 5-day, and 10-day forward returns\n",
    "    df_filtered[f'future_cumret_{h}d'] = df_filtered.groupby('permno')['retx'].shift(-h).rolling(h).sum().reset_index(level=0, drop=True)\n",
    "\n",
    "# Analyze mean future returns after extreme deviations\n",
    "long_returns = df_filtered[df_filtered['retx_relative'] <= lower_threshold][[f'future_cumret_{h}d' for h in [1, 3, 5, 10]]].mean()\n",
    "short_returns = df_filtered[df_filtered['retx_relative'] >= upper_threshold][[f'future_cumret_{h}d' for h in [1, 3, 5, 10]]].mean()\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot([1, 3, 5, 10], long_returns, marker='o', label='Long (Low retx_relative)')\n",
    "plt.plot([1, 3, 5, 10], -short_returns, marker='o', label='Short (High retx_relative)')\n",
    "plt.axhline(0, color='gray', linestyle='--')\n",
    "plt.xlabel(\"Holding Period (Days)\")\n",
    "plt.ylabel(\"Average Future Cumulative Return\")\n",
    "plt.title(\"Mean Reversion Analysis After Extreme Deviations\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe012fa-4b36-4ed5-a6ff-736d36b9a832",
   "metadata": {},
   "source": [
    "### üìä Findings\n",
    "\n",
    "The plot demonstrates that **mean reversion is not strongly observed** in the data:\n",
    "\n",
    "- **Long trades** (after underperformance) generate **positive returns** and is **profitable**, but the effect is weak.\n",
    "- **Short trades** (after outperformance) show **positive future returns**, meaning shorting these will generate **losses** as showin in the above graph. This indicates that previous winners tend to continue performing well.\n",
    "\n",
    "This suggests that the market dynamics in the analyzed period exhibit **momentum behavior**, where trends persist rather than reverse.\n",
    "\n",
    "\n",
    "### üìå Implications for Strategy\n",
    "\n",
    "- **Signal Caution:** The `retx_relative` variable alone is not sufficient to reliably capture profitable mean-reversion opportunities.\n",
    "- **Market Behavior:** Short-term momentum seems to dominate, especially within peer groups, reducing the effectiveness of a pure mean-reversion strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187b6a38-64c0-43b6-bd19-a7d5b5eac858",
   "metadata": {},
   "source": [
    "#### Lets only consider Low GARCH volatility periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8473a2-4f9c-4046-a5bc-71b103185796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define low volatility as bottom 30% of GARCH volatility (you can adjust this threshold)\n",
    "low_vol_threshold = df_filtered['garch_vol'].quantile(0.30)\n",
    "df_low_vol = df_filtered[df_filtered['garch_vol'] <= low_vol_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa517a4-0050-47a8-a405-c0141e2dd74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine thresholds for extreme deviations within low volatility periods\n",
    "lower_threshold_low_vol = df_low_vol['retx_relative'].quantile(0.05)\n",
    "upper_threshold_low_vol = df_low_vol['retx_relative'].quantile(0.95)\n",
    "\n",
    "# Create future return columns if not already created\n",
    "for h in [1, 3, 5, 10]:\n",
    "    df_low_vol[f'future_cumret_{h}d'] = (\n",
    "        df_low_vol.groupby('permno')['retx'].shift(-h).rolling(h).sum().reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "# Analyze mean future returns after extreme deviations in low volatility\n",
    "long_returns_low_vol = df_low_vol[df_low_vol['retx_relative'] <= lower_threshold_low_vol][[f'future_cumret_{h}d' for h in [1, 3, 5, 10]]].mean()\n",
    "short_returns_low_vol = df_low_vol[df_low_vol['retx_relative'] >= upper_threshold_low_vol][[f'future_cumret_{h}d' for h in [1, 3, 5, 10]]].mean()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot([1, 3, 5, 10], long_returns_low_vol, marker='o', label='Long (Low retx_relative)')\n",
    "plt.plot([1, 3, 5, 10], -short_returns_low_vol, marker='o', label='Short (High retx_relative)')  # Plotting profits\n",
    "plt.axhline(0, color='gray', linestyle='--')\n",
    "plt.xlabel(\"Holding Period (Days)\")\n",
    "plt.ylabel(\"Average Future Cumulative Return\")\n",
    "plt.title(\"Mean Reversion in Low GARCH Volatility Periods\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32681ade-9dcf-4f19-87bb-4299dd7e41a6",
   "metadata": {},
   "source": [
    "- Even in the low vol, our short position will be generating losses. Both the underperforming and outperforming stocks tend to move in upward direction. This shows no mean reversion in KNN based peer portfolios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f65bff1-a9c9-4615-b73e-78b82bc1c5f3",
   "metadata": {},
   "source": [
    "#### **Let's check on portfolios**\n",
    "Is there any group where mean reversion might work. and if yes, how many such group exist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5c08d5-ff74-41f8-8e61-871d2f60d36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_analysis_results = []\n",
    "\n",
    "for group_id, group_df in df_filtered.groupby('group_id'):\n",
    "    lower_threshold = group_df['retx_relative'].quantile(0.05)\n",
    "    upper_threshold = group_df['retx_relative'].quantile(0.95)\n",
    "    \n",
    "    long_future_return = group_df[group_df['retx_relative'] <= lower_threshold]['future_cumret_5d'].mean()\n",
    "    short_future_return = group_df[group_df['retx_relative'] >= upper_threshold]['future_cumret_5d'].mean()\n",
    "    \n",
    "    group_analysis_results.append({\n",
    "        'group_id': group_id,\n",
    "        'long_avg_return_5d': long_future_return,\n",
    "        'short_avg_return_5d': short_future_return\n",
    "    })\n",
    "\n",
    "group_analysis_df = pd.DataFrame(group_analysis_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be3d515-f40f-4f02-b5d9-7617533764bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean reversion should have long returns > 0 and short returns < 0\n",
    "mean_reversion_groups = group_analysis_df[\n",
    "    (group_analysis_df['long_avg_return_5d'] > 0) & \n",
    "    (group_analysis_df['short_avg_return_5d'] < 0)\n",
    "]\n",
    "\n",
    "print(f\"Number of groups where mean reversion works: {len(mean_reversion_groups)}\")\n",
    "display(mean_reversion_groups.sort_values(by='long_avg_return_5d', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba4c91a-edc8-4e44-b2d6-a05e8c31bfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_groups = group_analysis_df['group_id'].nunique()\n",
    "mean_reversion_groups_count = len(mean_reversion_groups)\n",
    "\n",
    "# Calculate percentage\n",
    "mean_reversion_percentage = (mean_reversion_groups_count / total_groups) * 100\n",
    "\n",
    "print(f\"üìä Total Groups Analyzed: {total_groups}\")\n",
    "print(f\"‚úÖ Groups Showing Mean Reversion: {mean_reversion_groups_count}\")\n",
    "print(f\"üìà Percentage of Groups with Mean Reversion: {mean_reversion_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71866e2b-db8d-449e-82b1-59d9adaf319a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract year from group_id (assuming it's like '1976-Q1-07')\n",
    "mean_reversion_groups['year'] = mean_reversion_groups['group_id'].str[:4]\n",
    "\n",
    "# Calculate how many groups showed mean reversion per year\n",
    "mean_reversion_by_year = mean_reversion_groups.groupby('year').size().reset_index(name='mean_reversion_groups')\n",
    "\n",
    "# Also calculate total groups per year for percentage calculation\n",
    "group_analysis_df['year'] = group_analysis_df['group_id'].str[:4]\n",
    "total_groups_by_year = group_analysis_df.groupby('year').size().reset_index(name='total_groups')\n",
    "\n",
    "# Merge and calculate percentage\n",
    "yearly_stats = pd.merge(mean_reversion_by_year, total_groups_by_year, on='year')\n",
    "yearly_stats['mean_reversion_percentage'] = (yearly_stats['mean_reversion_groups'] / yearly_stats['total_groups']) * 100\n",
    "yearly_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca7e21c-6054-4f3a-b33f-3f4028ca83bc",
   "metadata": {},
   "source": [
    "There are only 17.68% group where mean reversion would work. There's also atleast 1 group every year where mean reversion would work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5d2225-e126-4c83-84fc-233b81f49073",
   "metadata": {},
   "source": [
    "### **Now, let's look for trend following strategy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8327cb-1f9d-4b71-981e-80a39714bab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define thresholds for identifying trending stocks\n",
    "lower_threshold = df_filtered['retx_relative'].quantile(0.05)  # Underperformers (trend-following short)\n",
    "upper_threshold = df_filtered['retx_relative'].quantile(0.95)  # Outperformers (trend-following long)\n",
    "\n",
    "# Ensure future returns are calculated\n",
    "for h in [1, 3, 5, 10]:\n",
    "    df_filtered[f'future_cumret_{h}d'] = (\n",
    "        df_filtered.groupby('permno')['retx'].shift(-h).rolling(h).sum().reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "# Analyze mean future returns after extreme performance (trend-following)\n",
    "long_returns_trend = df_filtered[df_filtered['retx_relative'] >= upper_threshold][[f'future_cumret_{h}d' for h in [1, 3, 5, 10]]].mean()\n",
    "short_returns_trend = df_filtered[df_filtered['retx_relative'] <= lower_threshold][[f'future_cumret_{h}d' for h in [1, 3, 5, 10]]].mean()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot([1, 3, 5, 10], long_returns_trend, marker='o', label='Long (Trend Followers - High retx_relative)')\n",
    "plt.plot([1, 3, 5, 10], -short_returns_trend, marker='o', label='Short (Trend Followers - Low retx_relative)')  # Profit perspective\n",
    "plt.axhline(0, color='gray', linestyle='--')\n",
    "plt.xlabel(\"Holding Period (Days)\")\n",
    "plt.ylabel(\"Average Future Cumulative Return\")\n",
    "plt.title(\"Trend-Following Analysis Based on retx_relative\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab24c70-05d6-4506-875b-0c672971f1e0",
   "metadata": {},
   "source": [
    "Even in trend following our short positions doesn't generate profit. Seems not working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e837a9c7-f2e8-415b-86be-4e1141a27aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_returns_trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5dc1bb-f355-4309-8e42-46da9e3c55d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
